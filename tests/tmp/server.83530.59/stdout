### Starting server for test 
93781:M 03 May 2025 23:04:56.123 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
93781:M 03 May 2025 23:04:56.125 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
93781:M 03 May 2025 23:04:56.127 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=93781, just started
93781:M 03 May 2025 23:04:56.127 * Configuration loaded
93781:M 03 May 2025 23:04:56.127 * Increased maximum number of open files to 10032 (it was originally set to 256).
93781:M 03 May 2025 23:04:56.127 * monotonic clock: POSIX clock_gettime
93781:M 03 May 2025 23:04:56.128 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 23640
 |###|   .+###############+.   |###|    PID: 93781                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

93781:M 03 May 2025 23:04:56.130 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
93781:M 03 May 2025 23:04:56.130 * No cluster configuration found, I'm 0355953e79c569a4e5e657cf95ccaac850f3a1f4
93781:M 03 May 2025 23:04:56.146 * Server initialized
93781:M 03 May 2025 23:04:56.149 * Ready to accept connections tcp
93781:M 03 May 2025 23:04:56.152 * Ready to accept connections unix
93781:M 03 May 2025 23:04:56.257 - Accepted 127.0.0.1:52419
93781:M 03 May 2025 23:04:56.258 - Client closed connection id=2 addr=127.0.0.1:52419 laddr=127.0.0.1:23640 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
93781:M 03 May 2025 23:04:56.267 - Accepted 127.0.0.1:52420
93781:M 03 May 2025 23:04:56.267 * Cluster meet 127.0.0.1:23639 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.267 * Cluster meet 127.0.0.1:23638 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.267 * Cluster meet 127.0.0.1:23637 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.268 * Cluster meet 127.0.0.1:23636 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.268 * Cluster meet 127.0.0.1:23635 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.268 * Cluster meet 127.0.0.1:23634 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.268 * Cluster meet 127.0.0.1:23633 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.268 * Cluster meet 127.0.0.1:23632 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.268 * Cluster meet 127.0.0.1:23631 (user request from 'id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= user=default lib-name= lib-ver=').
93781:M 03 May 2025 23:04:56.271 # Missing implement of connection type tls
93781:M 03 May 2025 23:04:56.368 - Accepting cluster node connection from 127.0.0.1:52445
93781:M 03 May 2025 23:04:56.368 * IP address for this node updated to 127.0.0.1
93781:M 03 May 2025 23:04:56.368 * Successfully completed handshake with cca93a04ed33746869c86c50fb722d75e6455a3c ()
93781:M 03 May 2025 23:04:56.368 * configEpoch collision with node cca93a04ed33746869c86c50fb722d75e6455a3c (). configEpoch set to 1
93781:M 03 May 2025 23:04:56.378 - Accepting cluster node connection from 127.0.0.1:52446
93781:M 03 May 2025 23:04:56.379 * Successfully completed handshake with d74093fa5cbee191e5f4875844632e4b1178a823 ()
93781:M 03 May 2025 23:04:56.389 - Accepting cluster node connection from 127.0.0.1:52449
93781:M 03 May 2025 23:04:56.389 * Successfully completed handshake with db91efb42d98480add759e79b4ee28dd04a3a2e0 ()
93781:M 03 May 2025 23:04:56.397 - Accepting cluster node connection from 127.0.0.1:52452
93781:M 03 May 2025 23:04:56.397 - Accepting cluster node connection from 127.0.0.1:52453
93781:M 03 May 2025 23:04:56.397 * Successfully completed handshake with 9a21967532f5555297f32e59a6d14132c2074d4c ()
93781:M 03 May 2025 23:04:56.403 * Successfully completed handshake with d56e5afcc74e6566efeca03c7251faf1ab21a425 ()
93781:M 03 May 2025 23:04:56.403 - Accepting cluster node connection from 127.0.0.1:52455
93781:M 03 May 2025 23:04:56.408 * Successfully completed handshake with 5c935c3f58573c3c0a91c32b85a9758afa06799e ()
93781:M 03 May 2025 23:04:56.440 - Accepting cluster node connection from 127.0.0.1:52477
93781:M 03 May 2025 23:04:56.440 * Successfully completed handshake with 5c328f9d2f54be47ee2e8999022778ff086b3624 ()
93781:M 03 May 2025 23:04:56.442 - Accepting cluster node connection from 127.0.0.1:52478
93781:M 03 May 2025 23:04:56.442 * Successfully completed handshake with 0edc1598c6f19b54b438275a7178c8e2fb86866e ()
93781:M 03 May 2025 23:04:56.453 - Accepting cluster node connection from 127.0.0.1:52480
93781:M 03 May 2025 23:04:56.453 * Successfully completed handshake with 2a61950352228d652e0cd28d607003203413edc3 ()
93781:M 03 May 2025 23:04:56.514 * configEpoch collision with node 2a61950352228d652e0cd28d607003203413edc3 (). configEpoch set to 2
93781:M 03 May 2025 23:04:57.204 - Accepted 127.0.0.1:52671
93781:M 03 May 2025 23:04:57.220 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is no longer primary of shard 1d78e2e0763c8560fbddbec241a38e7f3805fcca; removed all 0 slot(s) it used to own
93781:M 03 May 2025 23:04:57.227 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is now part of shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93781:M 03 May 2025 23:04:57.227 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is now a replica of node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93781:M 03 May 2025 23:04:57.245 * Replica 127.0.0.1:23635 asks for synchronization
93781:M 03 May 2025 23:04:57.248 * Full resync requested by replica 127.0.0.1:23635
93781:M 03 May 2025 23:04:57.253 * Replication backlog created, my new replication IDs are '40ff9ff6317fc11f17302067588c4e23429b58e6' and '0000000000000000000000000000000000000000'
93781:M 03 May 2025 23:04:57.255 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
93781:M 03 May 2025 23:04:57.255 * Background RDB transfer started by pid 93863 to pipe through parent process
93863:C 03 May 2025 23:04:57.255 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
93781:M 03 May 2025 23:04:57.260 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
93781:M 03 May 2025 23:04:57.276 * Node 0edc1598c6f19b54b438275a7178c8e2fb86866e () is no longer primary of shard 4fc121ae00c8fcb15da30f003ea199f2f8cfe457; removed all 0 slot(s) it used to own
93781:M 03 May 2025 23:04:57.278 * Node 0edc1598c6f19b54b438275a7178c8e2fb86866e () is now part of shard 7ca01ed584c7f696f5412b60ba3b31d3fccb3643
93781:M 03 May 2025 23:04:57.284 * Node 0edc1598c6f19b54b438275a7178c8e2fb86866e () is now a replica of node cca93a04ed33746869c86c50fb722d75e6455a3c () in shard 7ca01ed584c7f696f5412b60ba3b31d3fccb3643
93781:M 03 May 2025 23:04:57.307 * Background RDB transfer terminated with success
93781:M 03 May 2025 23:04:57.307 * Streamed RDB transfer with replica 127.0.0.1:23635 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
93781:M 03 May 2025 23:04:57.307 * Synchronization with replica 127.0.0.1:23635 succeeded
93781:M 03 May 2025 23:04:57.307 * Node d74093fa5cbee191e5f4875844632e4b1178a823 () is no longer primary of shard a2c0be6204a680811ccd3d5db82f6cdf886409a3; removed all 0 slot(s) it used to own
93781:M 03 May 2025 23:04:57.307 * Node d74093fa5cbee191e5f4875844632e4b1178a823 () is now part of shard 4aa4b960efe84440273421442c9b7de1b3bf0707
93781:M 03 May 2025 23:04:57.307 * Node d74093fa5cbee191e5f4875844632e4b1178a823 () is now a replica of node 9a21967532f5555297f32e59a6d14132c2074d4c () in shard 4aa4b960efe84440273421442c9b7de1b3bf0707
93781:M 03 May 2025 23:04:57.346 * Node 5c328f9d2f54be47ee2e8999022778ff086b3624 () is no longer primary of shard 79a63df7d6cad403d83b5acc455de387c1311a74; removed all 0 slot(s) it used to own
93781:M 03 May 2025 23:04:57.347 * Node 5c328f9d2f54be47ee2e8999022778ff086b3624 () is now part of shard 06916f579a38286ea2028a011368696032f6c8ac
93781:M 03 May 2025 23:04:57.349 * Node 5c328f9d2f54be47ee2e8999022778ff086b3624 () is now a replica of node 2a61950352228d652e0cd28d607003203413edc3 () in shard 06916f579a38286ea2028a011368696032f6c8ac
93781:M 03 May 2025 23:04:57.380 # DEBUG LOG: ========== I am primary 0 ==========
93781:M 03 May 2025 23:04:57.383 * Node db91efb42d98480add759e79b4ee28dd04a3a2e0 () is no longer primary of shard 809fefde03e09cb34c1e2b85c6e639e885dc16db; removed all 0 slot(s) it used to own
93781:M 03 May 2025 23:04:57.387 * Node db91efb42d98480add759e79b4ee28dd04a3a2e0 () is now part of shard 54d1839130debb218475572bcaa4e761f3b9b2e4
93781:M 03 May 2025 23:04:57.391 * Node db91efb42d98480add759e79b4ee28dd04a3a2e0 () is now a replica of node d56e5afcc74e6566efeca03c7251faf1ab21a425 () in shard 54d1839130debb218475572bcaa4e761f3b9b2e4
93781:M 03 May 2025 23:04:58.198 * Cluster state changed: ok
### Starting test Cluster is up in tests/unit/cluster/update-msg.tcl
### Starting test Cluster is writable in tests/unit/cluster/update-msg.tcl
93781:M 03 May 2025 23:05:06.848 - Accepted 127.0.0.1:56061
93781:M 03 May 2025 23:05:06.848 - Client closed connection id=13 addr=127.0.0.1:56061 laddr=127.0.0.1:23640 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=cluster|nodes user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=28 tot-net-out=1244 tot-cmds=1
93781:M 03 May 2025 23:05:06.850 - Accepted 127.0.0.1:56071
93781:M 03 May 2025 23:05:06.895 - Client closed connection id=14 addr=127.0.0.1:56071 laddr=127.0.0.1:23640 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=30 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=1831 tot-net-out=802 tot-cmds=46
### Starting test Instance #5 is a slave in tests/unit/cluster/update-msg.tcl
### Starting test Instance #5 synced with the master in tests/unit/cluster/update-msg.tcl
### Starting test Killing one master node in tests/unit/cluster/update-msg.tcl
### Starting test Wait for failover in tests/unit/cluster/update-msg.tcl
### Starting test Cluster should eventually be up again in tests/unit/cluster/update-msg.tcl
### Starting test Cluster is writable in tests/unit/cluster/update-msg.tcl
### Starting test Instance #5 is now a master in tests/unit/cluster/update-msg.tcl
### Starting test Killing the new master #5 in tests/unit/cluster/update-msg.tcl
### Starting test Cluster should be down now in tests/unit/cluster/update-msg.tcl
### Starting test Restarting the old master node in tests/unit/cluster/update-msg.tcl
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57513
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57517
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57518
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57532
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57576
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57577
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57578
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57579
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:57633
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:60648
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:60655
93781:M 03 May 2025 23:05:14.696 - Accepting cluster node connection from 127.0.0.1:60693
93781:M 03 May 2025 23:05:14.697 - Accepting cluster node connection from 127.0.0.1:60750
93781:M 03 May 2025 23:05:14.697 - Accepting cluster node connection from 127.0.0.1:60751
93781:M 03 May 2025 23:05:14.697 - Accepting cluster node connection from 127.0.0.1:60752
93781:M 03 May 2025 23:05:14.697 - Accepting cluster node connection from 127.0.0.1:60753
93781:M 03 May 2025 23:05:14.697 - Accepting cluster node connection from 127.0.0.1:60807
93781:M 03 May 2025 23:05:14.697 - Accepted 127.0.0.1:60034
93781:M 03 May 2025 23:05:14.697 - Client closed connection id=11 addr=127.0.0.1:52671 laddr=127.0.0.1:23640 fd=33 name= age=17 idle=0 flags=S capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=1 omem=16920 tot-mem=35352 events=r cmd=replconf user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=816 tot-net-out=1323 tot-cmds=21
### Starting test Instance #0 gets converted into a slave in tests/unit/cluster/update-msg.tcl
93781:M 03 May 2025 23:05:14.697 * Connection with replica 127.0.0.1:23635 lost.
93781:M 03 May 2025 23:05:14.697 # Failover auth denied to 5c935c3f58573c3c0a91c32b85a9758afa06799e () for epoch 10: its primary is up
93781:M 03 May 2025 23:05:14.697 * Configuration change detected. Reconfiguring myself as a replica of node 5c935c3f58573c3c0a91c32b85a9758afa06799e () in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93781:S 03 May 2025 23:05:14.697 * Before turning into a replica, using my own primary parameters to synthesize a cached primary: I may be able to synchronize with the new primary with just a partial transfer.
93781:S 03 May 2025 23:05:14.697 * Connecting to PRIMARY 127.0.0.1:23635
93781:S 03 May 2025 23:05:14.697 * PRIMARY <-> REPLICA sync started
93781:S 03 May 2025 23:05:14.698 * FAIL message received from 5c328f9d2f54be47ee2e8999022778ff086b3624 () about 5c935c3f58573c3c0a91c32b85a9758afa06799e ()
93781:S 03 May 2025 23:05:14.698 - Client closed connection id=15 addr=127.0.0.1:60034 laddr=127.0.0.1:23640 fd=42 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=NULL user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=0 tot-net-out=0 tot-cmds=0
93781:S 03 May 2025 23:05:14.698 # Cluster state changed: fail
93781:S 03 May 2025 23:05:14.698 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
93781:S 03 May 2025 23:05:14.705 * Non blocking connect for SYNC fired the event.
### Starting test Restarting the new master node in tests/unit/cluster/update-msg.tcl
### Starting test Cluster is up again in tests/unit/cluster/update-msg.tcl
93781:S 03 May 2025 23:05:14.760 * Primary replied to PING, replication can continue...
93781:S 03 May 2025 23:05:14.761 * Trying a partial resynchronization (request 40ff9ff6317fc11f17302067588c4e23429b58e6:1297).
93781:S 03 May 2025 23:05:14.762 * Successful partial resynchronization with primary.
93781:S 03 May 2025 23:05:14.762 * Primary replication ID changed to 433f36e197a058a54bb40d504cb784e93a606136
93781:S 03 May 2025 23:05:14.762 * PRIMARY <-> REPLICA sync: Primary accepted a Partial Resynchronization.
93781:S 03 May 2025 23:05:14.797 * Start of election delayed for 819 milliseconds (rank #0, primary rank #0, offset 2578).
93781:S 03 May 2025 23:05:14.824 - Accepting cluster node connection from 127.0.0.1:63289
93781:S 03 May 2025 23:05:14.888 * Node cca93a04ed33746869c86c50fb722d75e6455a3c () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93781:S 03 May 2025 23:05:14.906 * Currently unable to failover: Waiting the delay before I can start a new failover.
93781:S 03 May 2025 23:05:14.909 * Node 9a21967532f5555297f32e59a6d14132c2074d4c () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93781:S 03 May 2025 23:05:15.007 * Currently unable to failover: Waiting the delay before I can start a new failover.
93781:S 03 May 2025 23:05:15.039 * Node 2a61950352228d652e0cd28d607003203413edc3 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93781:S 03 May 2025 23:05:15.176 * Node d56e5afcc74e6566efeca03c7251faf1ab21a425 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93781:S 03 May 2025 23:05:15.715 * Starting a failover election for epoch 11, node config epoch is 10
93781:S 03 May 2025 23:05:15.762 * Currently unable to failover: Waiting for votes, but majority still not reached.
93781:S 03 May 2025 23:05:15.762 * Needed quorum: 3. Number of votes received so far: 1
93781:S 03 May 2025 23:05:15.765 * Failover election won: I'm the new primary.
93781:S 03 May 2025 23:05:15.768 * configEpoch set to 11 after successful failover
93781:S 03 May 2025 23:05:15.768 * Setting myself to primary in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8 after failover; my old primary is 5c935c3f58573c3c0a91c32b85a9758afa06799e ()
93781:M 03 May 2025 23:05:15.768 * Connection with primary lost.
93781:M 03 May 2025 23:05:15.768 * Caching the disconnected primary state.
93781:M 03 May 2025 23:05:15.768 * Discarding previously cached primary state.
93781:M 03 May 2025 23:05:15.768 * Setting secondary replication ID to 433f36e197a058a54bb40d504cb784e93a606136, valid up to offset: 2579. New replication ID is 24e82f8febdce3b1fa30cf41bbc1112b541369fc
93781:M 03 May 2025 23:05:15.768 * Cluster state changed: ok
93781:M 03 May 2025 23:05:15.781 - Accepted 127.0.0.1:63946
93781:M 03 May 2025 23:05:15.823 * A failover occurred in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8; node 5c935c3f58573c3c0a91c32b85a9758afa06799e () failed over to node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () with a config epoch of 11
93781:M 03 May 2025 23:05:15.828 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is now a replica of node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93781:M 03 May 2025 23:05:15.835 * Clear FAIL state for node 5c935c3f58573c3c0a91c32b85a9758afa06799e (): replica is reachable again.
93781:M 03 May 2025 23:05:15.842 * Replica 127.0.0.1:23635 asks for synchronization
93781:M 03 May 2025 23:05:15.845 * Partial resynchronization request from 127.0.0.1:23635 accepted. Sending 0 bytes of backlog starting from offset 2579.
93781:M 03 May 2025 23:05:15.969 * Node 2a61950352228d652e0cd28d607003203413edc3 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93781:M 03 May 2025 23:05:16.032 * Node 9a21967532f5555297f32e59a6d14132c2074d4c () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93781:M 03 May 2025 23:05:16.041 * Node cca93a04ed33746869c86c50fb722d75e6455a3c () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93781:M 03 May 2025 23:05:16.044 * Node d56e5afcc74e6566efeca03c7251faf1ab21a425 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93781:M 03 May 2025 23:05:16.085 - Client closed connection id=3 addr=127.0.0.1:52420 laddr=127.0.0.1:23640 fd=14 name= age=20 idle=1 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=2048 rbp=1024 obl=0 oll=0 omem=0 tot-mem=19456 events=r cmd=cluster|info user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=6476 tot-net-out=420691 tot-cmds=216
93781:signal-handler (1746281117) Received SIGTERM scheduling shutdown...
93781:M 03 May 2025 23:05:18.020 * User requested shutdown...
93781:M 03 May 2025 23:05:18.026 * 1 of 1 replicas are in sync when shutting down.
93781:M 03 May 2025 23:05:18.032 * Removing the pid file.
93781:M 03 May 2025 23:05:18.035 * Saving the cluster configuration file before exiting.
93781:M 03 May 2025 23:05:18.047 * Removing the unix socket file.
93781:M 03 May 2025 23:05:18.047 # Valkey is now ready to exit, bye bye...
