### Starting server for test 
94021:M 03 May 2025 23:04:58.501 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
94021:M 03 May 2025 23:04:58.503 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
94021:M 03 May 2025 23:04:58.512 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=94021, just started
94021:M 03 May 2025 23:04:58.516 * Configuration loaded
94021:M 03 May 2025 23:04:58.520 * Increased maximum number of open files to 10032 (it was originally set to 256).
94021:M 03 May 2025 23:04:58.522 * monotonic clock: POSIX clock_gettime
94021:M 03 May 2025 23:04:58.523 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 25646
 |###|   .+###############+.   |###|    PID: 94021                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

94021:M 03 May 2025 23:04:58.523 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
94021:M 03 May 2025 23:04:58.524 * No cluster configuration found, I'm 132625915118d530460e0bc9c2bf8c50e4efe6fe
94021:M 03 May 2025 23:04:58.531 * Server initialized
94021:M 03 May 2025 23:04:58.531 * Ready to accept connections tcp
94021:M 03 May 2025 23:04:58.531 * Ready to accept connections unix
94021:M 03 May 2025 23:04:58.653 - Accepted 127.0.0.1:52899
94021:M 03 May 2025 23:04:58.658 - Client closed connection id=2 addr=127.0.0.1:52899 laddr=127.0.0.1:25646 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
94021:M 03 May 2025 23:04:58.683 - Accepted 127.0.0.1:52901
94021:M 03 May 2025 23:04:58.683 * Cluster meet 127.0.0.1:25645 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.683 * Cluster meet 127.0.0.1:25644 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.684 * Cluster meet 127.0.0.1:25643 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.684 * Cluster meet 127.0.0.1:25642 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.684 * Cluster meet 127.0.0.1:25641 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.684 * Cluster meet 127.0.0.1:25640 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.684 * Cluster meet 127.0.0.1:25639 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.684 * Cluster meet 127.0.0.1:25638 (user request from 'id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= user=default lib-name= lib-ver=').
94021:M 03 May 2025 23:04:58.686 # Missing implement of connection type tls
94021:M 03 May 2025 23:04:58.745 - Accepting cluster node connection from 127.0.0.1:52915
94021:M 03 May 2025 23:04:58.746 * IP address for this node updated to 127.0.0.1
94021:M 03 May 2025 23:04:58.746 * Successfully completed handshake with 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 ()
94021:M 03 May 2025 23:04:58.758 - Accepting cluster node connection from 127.0.0.1:52916
94021:M 03 May 2025 23:04:58.758 - Accepting cluster node connection from 127.0.0.1:52917
94021:M 03 May 2025 23:04:58.758 * Successfully completed handshake with 4ff930e01affe2106176b50d1af5cd6bbf7a6462 ()
94021:M 03 May 2025 23:04:58.758 * configEpoch collision with node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 (). configEpoch set to 1
94021:M 03 May 2025 23:04:58.758 * Successfully completed handshake with f7af5f6d568cbc58e57822057e3ffe67f09c514d ()
94021:M 03 May 2025 23:04:58.776 - Accepting cluster node connection from 127.0.0.1:52918
94021:M 03 May 2025 23:04:58.776 * Successfully completed handshake with 69b67e3cd16549a48fb1805489fea1b89029b8b0 ()
94021:M 03 May 2025 23:04:58.792 - Accepting cluster node connection from 127.0.0.1:52919
94021:M 03 May 2025 23:04:58.792 * Successfully completed handshake with 539e03f3a54b5b310857d0bc20d0efc58e44b516 ()
94021:M 03 May 2025 23:04:58.795 - Accepting cluster node connection from 127.0.0.1:52920
94021:M 03 May 2025 23:04:58.805 * Successfully completed handshake with ffeb2e425353840e689b63343f3a86dd9160a350 ()
94021:M 03 May 2025 23:04:58.805 - Accepting cluster node connection from 127.0.0.1:52921
94021:M 03 May 2025 23:04:58.806 * Successfully completed handshake with a0fd490cf9723cee3cccd519109d4722bf859964 ()
94021:M 03 May 2025 23:04:58.830 - Accepting cluster node connection from 127.0.0.1:52922
94021:M 03 May 2025 23:04:58.830 * Successfully completed handshake with e5e04d1a171a8dea46269c46678b2b695adeef98 ()
94021:M 03 May 2025 23:04:59.418 - Accepted 127.0.0.1:53057
94021:M 03 May 2025 23:04:59.422 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is no longer primary of shard 24da2a8aea89efa9a2fdffc5931489628f33f81d; removed all 0 slot(s) it used to own
94021:M 03 May 2025 23:04:59.424 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is now part of shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:M 03 May 2025 23:04:59.424 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is now a replica of node 132625915118d530460e0bc9c2bf8c50e4efe6fe () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:M 03 May 2025 23:04:59.460 - Accepted 127.0.0.1:53058
94021:M 03 May 2025 23:04:59.460 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is no longer primary of shard 81f13117c58aa63e271c14c8eaf541b802d5c634; removed all 0 slot(s) it used to own
94021:M 03 May 2025 23:04:59.460 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is now part of shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:M 03 May 2025 23:04:59.460 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is now a replica of node 132625915118d530460e0bc9c2bf8c50e4efe6fe () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:M 03 May 2025 23:04:59.468 * Replica 127.0.0.1:25643 asks for synchronization
94021:M 03 May 2025 23:04:59.468 * Full resync requested by replica 127.0.0.1:25643
94021:M 03 May 2025 23:04:59.468 * Replication backlog created, my new replication IDs are 'ca41f158a8d44c5712108f9178944c170d6976e1' and '0000000000000000000000000000000000000000'
94021:M 03 May 2025 23:04:59.468 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
94021:M 03 May 2025 23:04:59.468 * Background RDB transfer started by pid 94104 to pipe through parent process
94104:C 03 May 2025 23:04:59.469 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
94021:M 03 May 2025 23:04:59.470 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
94021:M 03 May 2025 23:04:59.470 * Replica 127.0.0.1:25640 asks for synchronization
94021:M 03 May 2025 23:04:59.470 * Full resync requested by replica 127.0.0.1:25640
94021:M 03 May 2025 23:04:59.470 * Current BGSAVE has socket target. Waiting for next BGSAVE for SYNC
94021:M 03 May 2025 23:04:59.476 * Node 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 () is no longer primary of shard 8cd5286162bf8385161e7dc46c9b667742e2d497; removed all 0 slot(s) it used to own
94021:M 03 May 2025 23:04:59.476 * Node 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 () is now part of shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94021:M 03 May 2025 23:04:59.476 * Node 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 () is now a replica of node a0fd490cf9723cee3cccd519109d4722bf859964 () in shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94021:M 03 May 2025 23:04:59.476 * Node 539e03f3a54b5b310857d0bc20d0efc58e44b516 () is no longer primary of shard 61fc42c6623264f9314edde7e9e331590fe0176d; removed all 0 slot(s) it used to own
94021:M 03 May 2025 23:04:59.476 * Node 539e03f3a54b5b310857d0bc20d0efc58e44b516 () is now part of shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94021:M 03 May 2025 23:04:59.476 * Node 539e03f3a54b5b310857d0bc20d0efc58e44b516 () is now a replica of node a0fd490cf9723cee3cccd519109d4722bf859964 () in shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94021:M 03 May 2025 23:04:59.479 * Background RDB transfer terminated with success
94021:M 03 May 2025 23:04:59.479 * Streamed RDB transfer with replica 127.0.0.1:25643 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
94021:M 03 May 2025 23:04:59.479 * Synchronization with replica 127.0.0.1:25643 succeeded
94021:M 03 May 2025 23:04:59.479 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
94021:M 03 May 2025 23:04:59.480 * Background RDB transfer started by pid 94105 to pipe through parent process
94105:C 03 May 2025 23:04:59.481 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
94021:M 03 May 2025 23:04:59.482 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
94021:M 03 May 2025 23:04:59.485 * Node ffeb2e425353840e689b63343f3a86dd9160a350 () is no longer primary of shard e29b73940a49a08e3a8edc44d8b3e1b0da6bdeb7; removed all 0 slot(s) it used to own
94021:M 03 May 2025 23:04:59.487 * Node ffeb2e425353840e689b63343f3a86dd9160a350 () is now part of shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94021:M 03 May 2025 23:04:59.487 * Node ffeb2e425353840e689b63343f3a86dd9160a350 () is now a replica of node 69b67e3cd16549a48fb1805489fea1b89029b8b0 () in shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94021:M 03 May 2025 23:04:59.488 * Background RDB transfer terminated with success
94021:M 03 May 2025 23:04:59.488 * Streamed RDB transfer with replica 127.0.0.1:25640 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
94021:M 03 May 2025 23:04:59.488 * Synchronization with replica 127.0.0.1:25640 succeeded
94021:M 03 May 2025 23:04:59.490 * Node f7af5f6d568cbc58e57822057e3ffe67f09c514d () is no longer primary of shard 89a8054a434e0f9cdc29c9f18862d2a67d9c68d9; removed all 0 slot(s) it used to own
94021:M 03 May 2025 23:04:59.492 * Node f7af5f6d568cbc58e57822057e3ffe67f09c514d () is now part of shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94021:M 03 May 2025 23:04:59.492 * Node f7af5f6d568cbc58e57822057e3ffe67f09c514d () is now a replica of node 69b67e3cd16549a48fb1805489fea1b89029b8b0 () in shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94021:M 03 May 2025 23:04:59.493 # DEBUG LOG: ========== I am primary 0 ==========
94021:M 03 May 2025 23:05:00.569 * Cluster state changed: ok
### Starting test Cluster is up in tests/unit/cluster/failover.tcl
### Starting test Cluster is writable in tests/unit/cluster/failover.tcl
94021:M 03 May 2025 23:05:09.187 - Accepted 127.0.0.1:58090
94021:M 03 May 2025 23:05:09.189 - Client closed connection id=11 addr=127.0.0.1:58090 laddr=127.0.0.1:25646 fd=33 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=cluster|nodes user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=28 tot-net-out=1162 tot-cmds=1
94021:M 03 May 2025 23:05:09.192 - Accepted 127.0.0.1:58092
94021:M 03 May 2025 23:05:09.220 - Client closed connection id=12 addr=127.0.0.1:58092 laddr=127.0.0.1:25646 fd=33 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=3105 tot-net-out=1360 tot-cmds=78
### Starting test Killing the first primary node in tests/unit/cluster/failover.tcl
### Starting test Wait for failover in tests/unit/cluster/failover.tcl
### Starting test Cluster should eventually be up again in tests/unit/cluster/failover.tcl
### Starting test Restarting the previously killed primary node in tests/unit/cluster/failover.tcl
94021:M 03 May 2025 23:05:13.591 - Accepting cluster node connection from 127.0.0.1:59839
94021:M 03 May 2025 23:05:13.597 - Accepting cluster node connection from 127.0.0.1:59928
94021:M 03 May 2025 23:05:13.599 - Accepting cluster node connection from 127.0.0.1:59929
94021:M 03 May 2025 23:05:13.602 - Accepting cluster node connection from 127.0.0.1:59930
94021:M 03 May 2025 23:05:13.605 - Accepting cluster node connection from 127.0.0.1:59931
94021:M 03 May 2025 23:05:13.611 - Accepting cluster node connection from 127.0.0.1:59932
94021:M 03 May 2025 23:05:13.620 - Accepting cluster node connection from 127.0.0.1:59933
### Starting test Instance #0 gets converted into a replica in tests/unit/cluster/failover.tcl
94021:M 03 May 2025 23:05:13.623 - Client closed connection id=9 addr=127.0.0.1:53058 laddr=127.0.0.1:25646 fd=32 name= age=14 idle=0 flags=S capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=1 omem=16920 tot-mem=35352 events=r cmd=replconf user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=781 tot-net-out=2199 tot-cmds=20
94021:M 03 May 2025 23:05:13.623 * Connection with replica 127.0.0.1:25640 lost.
94021:M 03 May 2025 23:05:13.624 - Client closed connection id=8 addr=127.0.0.1:53057 laddr=127.0.0.1:25646 fd=31 name= age=14 idle=0 flags=S capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=1 omem=16920 tot-mem=35352 events=r cmd=replconf user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=816 tot-net-out=2199 tot-cmds=21
94021:M 03 May 2025 23:05:13.626 * Connection with replica 127.0.0.1:25643 lost.
94021:M 03 May 2025 23:05:13.632 * Configuration change detected. Reconfiguring myself as a replica of node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:S 03 May 2025 23:05:13.636 * Before turning into a replica, using my own primary parameters to synthesize a cached primary: I may be able to synchronize with the new primary with just a partial transfer.
94021:S 03 May 2025 23:05:13.636 * Connecting to PRIMARY 127.0.0.1:25643
94021:S 03 May 2025 23:05:13.639 * PRIMARY <-> REPLICA sync started
94021:S 03 May 2025 23:05:13.639 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is now a replica of node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:S 03 May 2025 23:05:13.639 * Ignore stale message from 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6; gossip config epoch: 1, current config epoch: 9
94021:S 03 May 2025 23:05:13.642 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is now a replica of node 132625915118d530460e0bc9c2bf8c50e4efe6fe () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94021:S 03 May 2025 23:05:13.648 * Non blocking connect for SYNC fired the event.
94021:S 03 May 2025 23:05:13.649 * Primary replied to PING, replication can continue...
94021:S 03 May 2025 23:05:13.649 * Trying a partial resynchronization (request ca41f158a8d44c5712108f9178944c170d6976e1:2173).
94021:S 03 May 2025 23:05:13.650 * Successful partial resynchronization with primary.
94021:S 03 May 2025 23:05:13.650 * Primary replication ID changed to 0c367e3d741e9bde479796b41498d68a45d834f3
94021:S 03 May 2025 23:05:13.650 * PRIMARY <-> REPLICA sync: Primary accepted a Partial Resynchronization.
### Starting test Make sure the replicas always get the different ranks in tests/unit/cluster/failover.tcl
94021:S 03 May 2025 23:05:13.831 - Client closed connection id=3 addr=127.0.0.1:52901 laddr=127.0.0.1:25646 fd=14 name= age=15 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=2048 rbp=1024 obl=0 oll=0 omem=0 tot-mem=19456 events=r cmd=cluster|slots user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=6084 tot-net-out=371017 tot-cmds=202
94021:signal-handler (1746281116) Received SIGTERM scheduling shutdown...
94021:S 03 May 2025 23:05:16.123 * User requested shutdown...
94021:S 03 May 2025 23:05:16.128 * Removing the pid file.
94021:S 03 May 2025 23:05:16.128 * Saving the cluster configuration file before exiting.
94021:S 03 May 2025 23:05:16.133 * Removing the unix socket file.
94021:S 03 May 2025 23:05:16.133 # Valkey is now ready to exit, bye bye...
