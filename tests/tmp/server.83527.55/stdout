### Starting server for test 
93060:M 03 May 2025 23:04:52.148 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
93060:M 03 May 2025 23:04:52.148 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
93060:M 03 May 2025 23:04:52.148 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=93060, just started
93060:M 03 May 2025 23:04:52.148 * Configuration loaded
93060:M 03 May 2025 23:04:52.148 * Increased maximum number of open files to 10032 (it was originally set to 256).
93060:M 03 May 2025 23:04:52.148 * monotonic clock: POSIX clock_gettime
93060:M 03 May 2025 23:04:52.149 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 22138
 |###|   .+###############+.   |###|    PID: 93060                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

93060:M 03 May 2025 23:04:52.149 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
93060:M 03 May 2025 23:04:52.149 * No cluster configuration found, I'm cfd7b2eac3006d20de4d8819d890a0def1578f55
93060:M 03 May 2025 23:04:52.154 * Server initialized
93060:M 03 May 2025 23:04:52.154 * Ready to accept connections tcp
93060:M 03 May 2025 23:04:52.154 * Ready to accept connections unix
93060:M 03 May 2025 23:04:52.286 - Accepted 127.0.0.1:51196
93060:M 03 May 2025 23:04:52.286 - Client closed connection id=2 addr=127.0.0.1:51196 laddr=127.0.0.1:22138 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
93060:M 03 May 2025 23:04:52.310 - Accepted 127.0.0.1:51201
93060:M 03 May 2025 23:04:52.544 - Accepting cluster node connection from 127.0.0.1:51279
93060:M 03 May 2025 23:04:52.544 * IP address for this node updated to 127.0.0.1
93060:M 03 May 2025 23:04:52.554 # Missing implement of connection type tls
93060:M 03 May 2025 23:04:52.666 * configEpoch collision with node ef1e567d8f1512b87be09ea15d7c35e6bf67fb7e (). configEpoch set to 2
93060:M 03 May 2025 23:04:52.714 - Accepting cluster node connection from 127.0.0.1:51364
93060:M 03 May 2025 23:04:52.720 - Accepting cluster node connection from 127.0.0.1:51373
93060:M 03 May 2025 23:04:52.720 - Accepting cluster node connection from 127.0.0.1:51381
93060:M 03 May 2025 23:04:52.777 - Accepting cluster node connection from 127.0.0.1:51400
93060:M 03 May 2025 23:04:52.778 - Accepting cluster node connection from 127.0.0.1:51416
93060:M 03 May 2025 23:04:52.814 - Accepting cluster node connection from 127.0.0.1:51427
93060:M 03 May 2025 23:04:52.818 - Accepting cluster node connection from 127.0.0.1:51439
93060:M 03 May 2025 23:04:52.820 * Successfully completed handshake with 5cd2dce60033fcc4ef63956b3ed3b6b352d7a502 ()
93060:M 03 May 2025 23:04:52.878 - Accepting cluster node connection from 127.0.0.1:51473
93060:M 03 May 2025 23:04:52.925 * Successfully completed handshake with 00c5c3544a9cb7e11c28446fb70f8c5370ba2215 ()
93060:M 03 May 2025 23:04:53.127 * Node aa25892320b9f05c95ecf5b36e0311ba1c846583 () is no longer primary of shard 54ee3fe575ecb15f700bbbb591bb77ea969cb8ef; removed all 0 slot(s) it used to own
93060:M 03 May 2025 23:04:53.127 * Node aa25892320b9f05c95ecf5b36e0311ba1c846583 () is now part of shard e336a9a27041d0cee4788e5e75eb1a47b8b18d0d
93060:M 03 May 2025 23:04:53.127 * Node aa25892320b9f05c95ecf5b36e0311ba1c846583 () is now a replica of node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 () in shard e336a9a27041d0cee4788e5e75eb1a47b8b18d0d
93060:M 03 May 2025 23:04:53.128 - Accepted 127.0.0.1:51568
93060:M 03 May 2025 23:04:53.129 * Node ef1e567d8f1512b87be09ea15d7c35e6bf67fb7e () is no longer primary of shard 7cf2bdde6a39a0795e54c8aa3a6bee33710bb1b3; removed all 0 slot(s) it used to own
93060:M 03 May 2025 23:04:53.129 * Node ef1e567d8f1512b87be09ea15d7c35e6bf67fb7e () is now part of shard 7ec9c76be688682321f9cc6432d8fe3e310ee512
93060:M 03 May 2025 23:04:53.129 * Node ef1e567d8f1512b87be09ea15d7c35e6bf67fb7e () is now a replica of node cfd7b2eac3006d20de4d8819d890a0def1578f55 () in shard 7ec9c76be688682321f9cc6432d8fe3e310ee512
93060:M 03 May 2025 23:04:53.130 * Node 35e3684d330845aab3b3b8c8222bd0cf0426307c () is no longer primary of shard 8873cbe0689874410bf3bba4b9238e4254967575; removed all 0 slot(s) it used to own
93060:M 03 May 2025 23:04:53.130 * Node 35e3684d330845aab3b3b8c8222bd0cf0426307c () is now part of shard 51e5d3f7a3a912a417bfe216c1d6e8258e66a760
93060:M 03 May 2025 23:04:53.130 * Node 35e3684d330845aab3b3b8c8222bd0cf0426307c () is now a replica of node 5fddd6dd16d05f5149c7627c89ab22057d4fdf10 () in shard 51e5d3f7a3a912a417bfe216c1d6e8258e66a760
93060:M 03 May 2025 23:04:53.130 * Replica 127.0.0.1:22133 asks for synchronization
93060:M 03 May 2025 23:04:53.130 * Full resync requested by replica 127.0.0.1:22133
93060:M 03 May 2025 23:04:53.131 * Replication backlog created, my new replication IDs are '5608d0b1a41b37441b54765d66e5e431907659b6' and '0000000000000000000000000000000000000000'
93060:M 03 May 2025 23:04:53.131 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
93060:M 03 May 2025 23:04:53.132 * Background RDB transfer started by pid 93208 to pipe through parent process
93208:C 03 May 2025 23:04:53.133 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
93060:M 03 May 2025 23:04:53.135 * Node 5cd2dce60033fcc4ef63956b3ed3b6b352d7a502 () is no longer primary of shard 3d01b816cd114fb41f7fe30be6be458e04b28c99; removed all 0 slot(s) it used to own
93060:M 03 May 2025 23:04:53.135 * Node 5cd2dce60033fcc4ef63956b3ed3b6b352d7a502 () is now part of shard eddbf15e265e3341bb8bb5f23e63c9c9acae6ad3
93060:M 03 May 2025 23:04:53.135 * Node 5cd2dce60033fcc4ef63956b3ed3b6b352d7a502 () is now a replica of node 7d3399d6d71b9f1addc72ea3a10085ef56a0dade () in shard eddbf15e265e3341bb8bb5f23e63c9c9acae6ad3
93060:M 03 May 2025 23:04:53.135 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
93060:M 03 May 2025 23:04:53.139 * Node 00c5c3544a9cb7e11c28446fb70f8c5370ba2215 () is no longer primary of shard 7c3cb3ee4781c865ef9e69ed9ac97a5dfa83b007; removed all 0 slot(s) it used to own
93060:M 03 May 2025 23:04:53.139 * Node 00c5c3544a9cb7e11c28446fb70f8c5370ba2215 () is now part of shard 8887c6c8a455d9b1b8e2ffdede6989e883efac52
93060:M 03 May 2025 23:04:53.144 * Node 00c5c3544a9cb7e11c28446fb70f8c5370ba2215 () is now a replica of node 6cc182f5fd9d241f33f71e47c38b512e9b5263ad () in shard 8887c6c8a455d9b1b8e2ffdede6989e883efac52
93060:M 03 May 2025 23:04:53.145 * Background RDB transfer terminated with success
93060:M 03 May 2025 23:04:53.145 * Streamed RDB transfer with replica 127.0.0.1:22133 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
93060:M 03 May 2025 23:04:53.145 * Synchronization with replica 127.0.0.1:22133 succeeded
93060:M 03 May 2025 23:04:53.145 # DEBUG LOG: ========== I am primary 1 ==========
93060:M 03 May 2025 23:04:54.166 * Cluster state changed: ok
### Starting test Cluster is up in tests/unit/cluster/manual-failover.tcl
### Starting test Cluster is writable in tests/unit/cluster/manual-failover.tcl
93060:M 03 May 2025 23:05:02.935 - Accepted 127.0.0.1:53920
93060:M 03 May 2025 23:05:02.959 - Client closed connection id=10 addr=127.0.0.1:53920 laddr=127.0.0.1:22138 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=30 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=1671 tot-net-out=732 tot-cmds=42
### Starting test Instance #5 is a slave in tests/unit/cluster/manual-failover.tcl
### Starting test Instance #5 synced with the master in tests/unit/cluster/manual-failover.tcl
93060:M 03 May 2025 23:05:02.988 - Accepted 127.0.0.1:53972
### Starting test Send CLUSTER FAILOVER to #5, during load in tests/unit/cluster/manual-failover.tcl
93060:M 03 May 2025 23:05:03.483 * Failover auth granted to aa25892320b9f05c95ecf5b36e0311ba1c846583 () for epoch 10
93060:M 03 May 2025 23:05:03.560 * A failover occurred in shard e336a9a27041d0cee4788e5e75eb1a47b8b18d0d; node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 () failed over to node aa25892320b9f05c95ecf5b36e0311ba1c846583 () with a config epoch of 10
93060:M 03 May 2025 23:05:03.560 * Node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 () is now a replica of node aa25892320b9f05c95ecf5b36e0311ba1c846583 () in shard e336a9a27041d0cee4788e5e75eb1a47b8b18d0d
93060:M 03 May 2025 23:05:03.561 - Accepted 127.0.0.1:54231
93060:M 03 May 2025 23:05:03.562 - Client closed connection id=11 addr=127.0.0.1:53972 laddr=127.0.0.1:22138 fd=34 name= age=1 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=5 obl=0 oll=0 omem=0 tot-mem=18432 events=r cmd=rpush user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=90681 tot-net-out=4223 tot-cmds=1409
### Starting test Wait for failover in tests/unit/cluster/manual-failover.tcl
### Starting test Cluster should eventually be up again in tests/unit/cluster/manual-failover.tcl
### Starting test Cluster is writable in tests/unit/cluster/manual-failover.tcl
93060:M 03 May 2025 23:05:04.115 - Accepted 127.0.0.1:54512
93060:M 03 May 2025 23:05:04.118 - Client closed connection id=14 addr=127.0.0.1:54512 laddr=127.0.0.1:22138 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=cluster|nodes user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=28 tot-net-out=1246 tot-cmds=1
93060:M 03 May 2025 23:05:04.118 - Accepted 127.0.0.1:54518
93060:M 03 May 2025 23:05:04.140 - Client closed connection id=15 addr=127.0.0.1:54518 laddr=127.0.0.1:22138 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=1671 tot-net-out=732 tot-cmds=42
### Starting test Instance #5 is now a master in tests/unit/cluster/manual-failover.tcl
### Starting test Verify 50000 keys for consistency with logical content in tests/unit/cluster/manual-failover.tcl
### Starting test Instance #0 gets converted into a slave in tests/unit/cluster/manual-failover.tcl
### Starting test Check for memory leaks (pid 93113) in tests/unit/cluster/manual-failover.tcl
93060:M 03 May 2025 23:05:05.769 - Connection with Node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 at 127.0.0.1:32139 failed: Connection refused
93060:M 03 May 2025 23:05:05.812 - Client closed connection id=3 addr=127.0.0.1:51201 laddr=127.0.0.1:22138 fd=14 name= age=13 idle=1 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=874 obl=0 oll=0 omem=0 tot-mem=18432 events=r cmd=cluster|slots user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=743 tot-net-out=22862 tot-cmds=24
93060:M 03 May 2025 23:05:05.869 - Connection with Node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 at 127.0.0.1:32139 failed: Connection refused
93060:M 03 May 2025 23:05:05.970 - Connection with Node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 at 127.0.0.1:32139 failed: Connection refused
93060:M 03 May 2025 23:05:06.878 - Connection with Node e9ab92522ac59e38f8ef7ecddcb23ed30aabf680 at 127.0.0.1:32139 failed: Connection refused
93060:signal-handler (1746281106) Received SIGTERM scheduling shutdown...
93060:M 03 May 2025 23:05:06.979 * User requested shutdown...
93060:M 03 May 2025 23:05:06.986 * 1 of 1 replicas are in sync when shutting down.
93060:M 03 May 2025 23:05:06.989 * Removing the pid file.
93060:M 03 May 2025 23:05:06.990 * Saving the cluster configuration file before exiting.
93060:M 03 May 2025 23:05:07.020 * Removing the unix socket file.
93060:M 03 May 2025 23:05:07.021 # Valkey is now ready to exit, bye bye...
