### Starting server for test 
5040:M 03 May 2025 23:06:37.670 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
5040:M 03 May 2025 23:06:37.670 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
5040:M 03 May 2025 23:06:37.670 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=5040, just started
5040:M 03 May 2025 23:06:37.670 * Configuration loaded
5040:M 03 May 2025 23:06:37.670 * Increased maximum number of open files to 10032 (it was originally set to 256).
5040:M 03 May 2025 23:06:37.670 * monotonic clock: POSIX clock_gettime
5040:M 03 May 2025 23:06:37.670 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 28151
 |###|   .+###############+.   |###|    PID: 5040                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

5040:M 03 May 2025 23:06:37.670 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
5040:M 03 May 2025 23:06:37.670 * No cluster configuration found, I'm f8557e569b17a624461729f78af03982b6d35259
5040:M 03 May 2025 23:06:37.677 * Server initialized
5040:M 03 May 2025 23:06:37.677 * Ready to accept connections tcp
5040:M 03 May 2025 23:06:37.677 * Ready to accept connections unix
5040:M 03 May 2025 23:06:37.821 - Accepted 127.0.0.1:62354
5040:M 03 May 2025 23:06:37.821 - Client closed connection id=2 addr=127.0.0.1:62354 laddr=127.0.0.1:28151 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
5040:M 03 May 2025 23:06:37.848 - Accepted 127.0.0.1:62357
5040:M 03 May 2025 23:06:39.060 - Accepting cluster node connection from 127.0.0.1:63248
5040:M 03 May 2025 23:06:39.060 * IP address for this node updated to 127.0.0.1
5040:M 03 May 2025 23:06:39.184 - Accepting cluster node connection from 127.0.0.1:63349
5040:M 03 May 2025 23:06:39.190 * Successfully completed handshake with d67d7943c1deb3eee3c1900fd39405a367dbf29f ()
5040:M 03 May 2025 23:06:39.203 - Accepting cluster node connection from 127.0.0.1:63368
5040:M 03 May 2025 23:06:39.213 * Successfully completed handshake with 2cbd7ea25a13bbe4685d01b3dc9c208fc56aced2 ()
5040:M 03 May 2025 23:06:39.221 - Accepting cluster node connection from 127.0.0.1:63371
5040:M 03 May 2025 23:06:39.236 - Accepting cluster node connection from 127.0.0.1:63391
5040:M 03 May 2025 23:06:39.331 - Accepting cluster node connection from 127.0.0.1:63482
5040:M 03 May 2025 23:06:39.473 # Missing implement of connection type tls
5040:M 03 May 2025 23:06:39.474 * Node c6c765da8761077e56e716e56cf93c3d2d6f70ee () is no longer primary of shard 74bb32459afcc941d1efa0ad9cc919d5033c5b79; removed all 0 slot(s) it used to own
5040:M 03 May 2025 23:06:39.474 * Node c6c765da8761077e56e716e56cf93c3d2d6f70ee () is now part of shard 1b1f679f3abb9d71d1adb03e6490e990cced014d
5040:M 03 May 2025 23:06:39.474 * Node c6c765da8761077e56e716e56cf93c3d2d6f70ee () is now a replica of node c9e5a0b46135e8396d80e0c862f705be6af85ef4 () in shard 1b1f679f3abb9d71d1adb03e6490e990cced014d
5040:S 03 May 2025 23:06:39.474 * Connecting to PRIMARY 127.0.0.1:28157
5040:S 03 May 2025 23:06:39.474 * PRIMARY <-> REPLICA sync started
5040:S 03 May 2025 23:06:39.474 * Cluster state changed: ok
5040:S 03 May 2025 23:06:39.475 * Non blocking connect for SYNC fired the event.
5040:S 03 May 2025 23:06:39.477 * Node 2cbd7ea25a13bbe4685d01b3dc9c208fc56aced2 () is no longer primary of shard c435f9a2982aa65f5dc900c7712cc7e0acb3b964; removed all 0 slot(s) it used to own
5040:S 03 May 2025 23:06:39.477 * Node 2cbd7ea25a13bbe4685d01b3dc9c208fc56aced2 () is now part of shard dbed2148a2a919b523dc119cd1bb807fb422a5e4
5040:S 03 May 2025 23:06:39.477 * Node 2cbd7ea25a13bbe4685d01b3dc9c208fc56aced2 () is now a replica of node 9e34aa65a57450b0b78b3ff60126f2dba15432b3 () in shard dbed2148a2a919b523dc119cd1bb807fb422a5e4
5040:S 03 May 2025 23:06:39.477 * Primary replied to PING, replication can continue...
5040:S 03 May 2025 23:06:39.479 * Partial resynchronization not possible (no cached primary)
5040:S 03 May 2025 23:06:39.480 * Node d67d7943c1deb3eee3c1900fd39405a367dbf29f () is no longer primary of shard 88fcd9108de6997b5ab78e26df6881f3bfa26f59; removed all 0 slot(s) it used to own
5040:S 03 May 2025 23:06:39.480 * Node d67d7943c1deb3eee3c1900fd39405a367dbf29f () is now part of shard e83bbd260e839860b36f89307e08031b8523451b
5040:S 03 May 2025 23:06:39.480 * Node d67d7943c1deb3eee3c1900fd39405a367dbf29f () is now a replica of node 89bc7eaa899d3a8e42e4ea195766c9319462b3ee () in shard e83bbd260e839860b36f89307e08031b8523451b
5040:S 03 May 2025 23:06:39.486 # DEBUG LOG: ========== I am replica 6 ==========
5040:S 03 May 2025 23:06:39.565 * Full resync from primary: a207c70e84630498b98851483e9b654c96aa8bdd:0
5040:S 03 May 2025 23:06:39.568 * PRIMARY <-> REPLICA sync: receiving streamed RDB from primary with EOF to disk
5040:S 03 May 2025 23:06:39.571 * PRIMARY <-> REPLICA sync: Flushing old data
5040:S 03 May 2025 23:06:39.571 * PRIMARY <-> REPLICA sync: Loading DB in memory
5040:S 03 May 2025 23:06:39.571 * Loading RDB produced by Valkey version 255.255.255
5040:S 03 May 2025 23:06:39.571 * RDB age 0 seconds
5040:S 03 May 2025 23:06:39.571 * RDB memory usage when created 2.99 Mb
5040:S 03 May 2025 23:06:39.571 * Done loading RDB, keys loaded: 0, keys expired: 0.
5040:S 03 May 2025 23:06:39.571 * PRIMARY <-> REPLICA sync: Finished with success
### Starting test auto-failover-on-shutdown hands over primaryship to a fully sync'd replica - sigterm - shutdown-timeout: 10 in tests/unit/cluster/auto-failover-on-shutdown.tcl
5040:S 03 May 2025 23:06:52.326 * NODE c6c765da8761077e56e716e56cf93c3d2d6f70ee () possibly failing.
5040:S 03 May 2025 23:06:52.409 * Node c9e5a0b46135e8396d80e0c862f705be6af85ef4 () reported node c6c765da8761077e56e716e56cf93c3d2d6f70ee () as not reachable.
5040:S 03 May 2025 23:06:52.410 * FAIL message received from 89bc7eaa899d3a8e42e4ea195766c9319462b3ee () about c6c765da8761077e56e716e56cf93c3d2d6f70ee ()
5040:S 03 May 2025 23:06:52.410 * Node 9e34aa65a57450b0b78b3ff60126f2dba15432b3 () reported node c6c765da8761077e56e716e56cf93c3d2d6f70ee () as not reachable.
5040:S 03 May 2025 23:06:52.491 * Node 89bc7eaa899d3a8e42e4ea195766c9319462b3ee () reported node c6c765da8761077e56e716e56cf93c3d2d6f70ee () as not reachable.
5040:S 03 May 2025 23:06:52.830 - DB 0: 1 keys (0 volatile) in 7 slots HT.
5040:S 03 May 2025 23:06:57.873 - DB 0: 1 keys (0 volatile) in 7 slots HT.
5040:S 03 May 2025 23:06:59.380 * Forced failover primary request accepted (primary request from 'id=5 addr=127.0.0.1:28157 laddr=127.0.0.1:63660 fd=27 name= user=(superuser) lib-name= lib-ver=').
5040:S 03 May 2025 23:06:59.380 * Start of election delayed for 0 milliseconds (rank #0, primary rank #0, offset 488).
5040:S 03 May 2025 23:06:59.380 * Starting a failover election for epoch 7, node config epoch is 1
5040:S 03 May 2025 23:06:59.393 - Client closed connection id=5 addr=127.0.0.1:28157 laddr=127.0.0.1:63660 fd=27 name= age=20 idle=0 flags=M capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=65530 argv-mem=0 multi-mem=0 rbs=1024 rbp=36 obl=0 oll=0 omem=0 tot-mem=67072 events=r cmd=cluster|failover user=(superuser) redir=-1 resp=2 lib-name= lib-ver= tot-net-in=488 tot-net-out=770 tot-cmds=14
5040:S 03 May 2025 23:06:59.395 * Connection with primary lost.
5040:S 03 May 2025 23:06:59.396 * Caching the disconnected primary state.
5040:S 03 May 2025 23:06:59.396 * Reconnecting to PRIMARY 127.0.0.1:28157
5040:S 03 May 2025 23:06:59.397 * PRIMARY <-> REPLICA sync started
5040:S 03 May 2025 23:06:59.397 * Currently unable to failover: Waiting for votes, but majority still not reached.
5040:S 03 May 2025 23:06:59.397 * Needed quorum: 2. Number of votes received so far: 0
5040:S 03 May 2025 23:06:59.397 # Error condition on socket for SYNC: Connection refused
5040:S 03 May 2025 23:06:59.399 - Connection with Node c9e5a0b46135e8396d80e0c862f705be6af85ef4 at 127.0.0.1:38157 failed: Connection refused
5040:S 03 May 2025 23:06:59.399 * Failover election won: I'm the new primary.
5040:S 03 May 2025 23:06:59.399 * configEpoch set to 7 after successful failover
5040:S 03 May 2025 23:06:59.399 * Setting myself to primary in shard 1b1f679f3abb9d71d1adb03e6490e990cced014d after failover; my old primary is c9e5a0b46135e8396d80e0c862f705be6af85ef4 ()
5040:M 03 May 2025 23:06:59.399 * Discarding previously cached primary state.
5040:M 03 May 2025 23:06:59.399 * Setting secondary replication ID to a207c70e84630498b98851483e9b654c96aa8bdd, valid up to offset: 489. New replication ID is 2e6312bdc56fa2309167c7b799bf6258e36b4b4e
### Starting test Unable to find a replica to perform an auto failover - sigterm in tests/unit/cluster/auto-failover-on-shutdown.tcl
5040:M 03 May 2025 23:06:59.462 - Accepted 127.0.0.1:61368
5040:M 03 May 2025 23:06:59.468 * Node c6c765da8761077e56e716e56cf93c3d2d6f70ee () is now a replica of node f8557e569b17a624461729f78af03982b6d35259 () in shard 1b1f679f3abb9d71d1adb03e6490e990cced014d
5040:M 03 May 2025 23:06:59.469 * Clear FAIL state for node c6c765da8761077e56e716e56cf93c3d2d6f70ee (): replica is reachable again.
5040:M 03 May 2025 23:06:59.469 * Node c6c765da8761077e56e716e56cf93c3d2d6f70ee () is now a replica of node c9e5a0b46135e8396d80e0c862f705be6af85ef4 () in shard 1b1f679f3abb9d71d1adb03e6490e990cced014d
5040:M 03 May 2025 23:06:59.470 * Replica 127.0.0.1:28154 asks for synchronization
5040:M 03 May 2025 23:06:59.470 * Partial resynchronization request from 127.0.0.1:28154 accepted. Sending 0 bytes of backlog starting from offset 489.
5040:M 03 May 2025 23:06:59.496 * Connection with replica 127.0.0.1:28154 lost.
5040:M 03 May 2025 23:06:59.497 - Connection with Node c9e5a0b46135e8396d80e0c862f705be6af85ef4 at 127.0.0.1:38157 failed: Connection refused
5040:signal-handler (1746281219) Received SIGTERM scheduling shutdown...
5040:M 03 May 2025 23:06:59.576 * Node 9e34aa65a57450b0b78b3ff60126f2dba15432b3 () reported node c6c765da8761077e56e716e56cf93c3d2d6f70ee () is back online.
5040:M 03 May 2025 23:06:59.598 * User requested shutdown...
5040:M 03 May 2025 23:06:59.598 * Removing the pid file.
5040:M 03 May 2025 23:06:59.598 * Unable to find a replica to perform the auto failover on shutdown.
5040:M 03 May 2025 23:06:59.598 * Saving the cluster configuration file before exiting.
5040:M 03 May 2025 23:06:59.605 * Removing the unix socket file.
5040:M 03 May 2025 23:06:59.605 # Valkey is now ready to exit, bye bye...
### Starting test Check for memory leaks (pid 5201) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 5179) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 5149) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 5115) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 5094) in tests/unit/cluster/auto-failover-on-shutdown.tcl
