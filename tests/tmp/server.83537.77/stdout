### Starting server for test 
95940:M 03 May 2025 23:05:19.204 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
95940:M 03 May 2025 23:05:19.208 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
95940:M 03 May 2025 23:05:19.213 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=95940, just started
95940:M 03 May 2025 23:05:19.216 * Configuration loaded
95940:M 03 May 2025 23:05:19.221 * Increased maximum number of open files to 10032 (it was originally set to 256).
95940:M 03 May 2025 23:05:19.227 * monotonic clock: POSIX clock_gettime
95940:M 03 May 2025 23:05:19.229 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 27149
 |###|   .+###############+.   |###|    PID: 95940                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

95940:M 03 May 2025 23:05:19.234 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
95940:M 03 May 2025 23:05:19.235 * No cluster configuration found, I'm b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3
95940:M 03 May 2025 23:05:19.245 * Server initialized
95940:M 03 May 2025 23:05:19.245 * Ready to accept connections tcp
95940:M 03 May 2025 23:05:19.245 * Ready to accept connections unix
95940:M 03 May 2025 23:05:19.373 - Accepted 127.0.0.1:49493
95940:M 03 May 2025 23:05:19.377 - Client closed connection id=2 addr=127.0.0.1:49493 laddr=127.0.0.1:27149 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
95940:M 03 May 2025 23:05:19.415 - Accepted 127.0.0.1:49548
95940:M 03 May 2025 23:05:21.267 # Cluster is currently down: I am part of a minority partition.
95940:M 03 May 2025 23:05:22.916 - Accepting cluster node connection from 127.0.0.1:52161
95940:M 03 May 2025 23:05:22.921 * IP address for this node updated to 127.0.0.1
95940:M 03 May 2025 23:05:23.184 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
95940:M 03 May 2025 23:05:23.262 - Accepting cluster node connection from 127.0.0.1:52598
95940:M 03 May 2025 23:05:23.268 - Accepting cluster node connection from 127.0.0.1:52679
95940:M 03 May 2025 23:05:23.275 - Accepting cluster node connection from 127.0.0.1:52756
95940:M 03 May 2025 23:05:23.281 - Accepting cluster node connection from 127.0.0.1:52769
95940:M 03 May 2025 23:05:23.286 - Accepting cluster node connection from 127.0.0.1:52770
95940:M 03 May 2025 23:05:23.333 - Accepting cluster node connection from 127.0.0.1:52803
95940:M 03 May 2025 23:05:23.341 - Accepting cluster node connection from 127.0.0.1:52822
95940:M 03 May 2025 23:05:23.349 * Successfully completed handshake with 0c687ad12155a843b2eb8ca14588185b8bffc837 ()
95940:M 03 May 2025 23:05:23.441 - Accepting cluster node connection from 127.0.0.1:52873
95940:M 03 May 2025 23:05:23.446 - Accepting cluster node connection from 127.0.0.1:52911
95940:M 03 May 2025 23:05:23.454 - Accepting cluster node connection from 127.0.0.1:52976
95940:M 03 May 2025 23:05:23.463 - Accepting cluster node connection from 127.0.0.1:52997
95940:M 03 May 2025 23:05:23.467 - Accepting cluster node connection from 127.0.0.1:53017
95940:M 03 May 2025 23:05:23.486 - Accepting cluster node connection from 127.0.0.1:53081
95940:M 03 May 2025 23:05:23.497 - Accepting cluster node connection from 127.0.0.1:53096
95940:M 03 May 2025 23:05:23.502 - Accepting cluster node connection from 127.0.0.1:53137
95940:M 03 May 2025 23:05:23.655 - Accepting cluster node connection from 127.0.0.1:53230
95940:M 03 May 2025 23:05:23.661 * configEpoch collision with node d647fe3c6249caeecf6b9af44adca3d1102dfa46 (). configEpoch set to 6
95940:M 03 May 2025 23:05:23.672 * Successfully completed handshake with 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca ()
95940:M 03 May 2025 23:05:25.190 # Missing implement of connection type tls
95940:M 03 May 2025 23:05:25.296 * Node 46671e5b4c343c9639452ff6da1989e7ac8280cd () is no longer primary of shard d23d6577021315023b1dd173d641d1926f907df3; removed all 0 slot(s) it used to own
95940:M 03 May 2025 23:05:25.303 * Node 46671e5b4c343c9639452ff6da1989e7ac8280cd () is now part of shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:05:25.307 * Node 46671e5b4c343c9639452ff6da1989e7ac8280cd () is now a replica of node 90fe983302002b82cb23590ee6d0f8941a6f292c () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.334 * Connecting to PRIMARY 127.0.0.1:27158
95940:S 03 May 2025 23:05:25.334 * PRIMARY <-> REPLICA sync started
95940:S 03 May 2025 23:05:25.343 * Cluster state changed: ok
95940:S 03 May 2025 23:05:25.348 * Non blocking connect for SYNC fired the event.
95940:S 03 May 2025 23:05:25.348 * Node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () is no longer primary of shard 2188c61d04f7f0a25193408f3d8746ab2f846352; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.348 * Node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () is now part of shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.348 * Node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () is now a replica of node 90fe983302002b82cb23590ee6d0f8941a6f292c () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.353 * Node b07f064c60c460967f75f99083ccb3194b9c7693 () is no longer primary of shard 5f5ffebf28e888e88191b6a587c99a11e281ceba; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.353 * Node b07f064c60c460967f75f99083ccb3194b9c7693 () is now part of shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.353 * Node b07f064c60c460967f75f99083ccb3194b9c7693 () is now a replica of node 90fe983302002b82cb23590ee6d0f8941a6f292c () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.356 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is no longer primary of shard 30120e5cb282a2877a7ad3d91cfcd1c147513111; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.356 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now part of shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.356 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now a replica of node 90fe983302002b82cb23590ee6d0f8941a6f292c () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:25.357 * Primary replied to PING, replication can continue...
95940:S 03 May 2025 23:05:25.359 * Partial resynchronization not possible (no cached primary)
95940:S 03 May 2025 23:05:25.364 * Node b9b46fb4ad2ec44f60cdb6c0ca604a7f1f256726 () is no longer primary of shard 609eb7b194545b6498d2677490de754944d2e8cb; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.364 * Node b9b46fb4ad2ec44f60cdb6c0ca604a7f1f256726 () is now part of shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.364 * Node b9b46fb4ad2ec44f60cdb6c0ca604a7f1f256726 () is now a replica of node b0460c495605a702515eee2bbc385790a6fbec4f () in shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.364 * Full resync from primary: 7d58ddc376db3b26eb97d51b67f28be16101cc7d:0
95940:S 03 May 2025 23:05:25.366 * Node bad1ce8ea8e5fe0a5d60190ac38361547ddadb8e () is no longer primary of shard bcaaa2ea0b6b303b1dc64e1fc492fde94bd35d92; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.366 * Node bad1ce8ea8e5fe0a5d60190ac38361547ddadb8e () is now part of shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.366 * Node bad1ce8ea8e5fe0a5d60190ac38361547ddadb8e () is now a replica of node b0460c495605a702515eee2bbc385790a6fbec4f () in shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.369 * PRIMARY <-> REPLICA sync: receiving streamed RDB from primary with EOF to disk
95940:S 03 May 2025 23:05:25.375 * PRIMARY <-> REPLICA sync: Flushing old data
95940:S 03 May 2025 23:05:25.375 * PRIMARY <-> REPLICA sync: Loading DB in memory
95940:S 03 May 2025 23:05:25.375 * Loading RDB produced by Valkey version 255.255.255
95940:S 03 May 2025 23:05:25.377 * RDB age 0 seconds
95940:S 03 May 2025 23:05:25.381 * RDB memory usage when created 3.12 Mb
95940:S 03 May 2025 23:05:25.385 * Done loading RDB, keys loaded: 0, keys expired: 0.
95940:S 03 May 2025 23:05:25.388 * PRIMARY <-> REPLICA sync: Finished with success
95940:S 03 May 2025 23:05:25.388 * Node 850a0b709f6fd994b2265ebf4364abdc3c616c13 () is no longer primary of shard 8d0dba8e2dd945d869c1c0541d2e2246f711b44c; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.388 * Node 850a0b709f6fd994b2265ebf4364abdc3c616c13 () is now part of shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.395 * Node 850a0b709f6fd994b2265ebf4364abdc3c616c13 () is now a replica of node b0460c495605a702515eee2bbc385790a6fbec4f () in shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.395 * Node 046e44a65cc052012ccd55765babd4aabb14ecc1 () is no longer primary of shard 70ab99f5bb9aae2f734265441e49f9496bbd0c0e; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.395 * Node 046e44a65cc052012ccd55765babd4aabb14ecc1 () is now part of shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.395 * Node 046e44a65cc052012ccd55765babd4aabb14ecc1 () is now a replica of node b0460c495605a702515eee2bbc385790a6fbec4f () in shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.407 * Node 8e266e9d6e33071bd4ebd8616c154d0d3e009dec () is no longer primary of shard 26c548505ec1ecf0597013251764b1726b388b98; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.408 * Node 8e266e9d6e33071bd4ebd8616c154d0d3e009dec () is now part of shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.408 * Node 8e266e9d6e33071bd4ebd8616c154d0d3e009dec () is now a replica of node b0460c495605a702515eee2bbc385790a6fbec4f () in shard 7b37de049781604aab4ac894d8d78f2fe3df2e69
95940:S 03 May 2025 23:05:25.416 * Node 5efe9ebd92f2876d4c3e37c7dc016ea9d86346eb () is no longer primary of shard 448817fb803293ad3a7fe962bd3cda16961d7fc1; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.417 * Node 5efe9ebd92f2876d4c3e37c7dc016ea9d86346eb () is now part of shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.417 * Node 5efe9ebd92f2876d4c3e37c7dc016ea9d86346eb () is now a replica of node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () in shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.424 * Node 0c687ad12155a843b2eb8ca14588185b8bffc837 () is no longer primary of shard edb4e4a1f986a0925e7570d317642aa8e4c26bb6; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.425 * Node 0c687ad12155a843b2eb8ca14588185b8bffc837 () is now part of shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.426 * Node 0c687ad12155a843b2eb8ca14588185b8bffc837 () is now a replica of node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () in shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.431 * Node 81588ad0024e11177626cca2c8ee4fd836e1ed14 () is no longer primary of shard ed53db1b4a03669a1cac0679827a0efe1a5c531d; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.434 * Node 81588ad0024e11177626cca2c8ee4fd836e1ed14 () is now part of shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.438 * Node 81588ad0024e11177626cca2c8ee4fd836e1ed14 () is now a replica of node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () in shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.508 * Node d647fe3c6249caeecf6b9af44adca3d1102dfa46 () is no longer primary of shard 8e6b5a893b6e53e9ed8274188595f8c8e267408d; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.511 * Node d647fe3c6249caeecf6b9af44adca3d1102dfa46 () is now part of shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.516 * Node d647fe3c6249caeecf6b9af44adca3d1102dfa46 () is now a replica of node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () in shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.524 * Node 071f788bd26ee028f2ea656a5a2139f79f12d6ad () is no longer primary of shard 7a87e0296504e5134f504882139bc2e3f76c4b89; removed all 0 slot(s) it used to own
95940:S 03 May 2025 23:05:25.528 * Node 071f788bd26ee028f2ea656a5a2139f79f12d6ad () is now part of shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.528 * Node 071f788bd26ee028f2ea656a5a2139f79f12d6ad () is now a replica of node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () in shard 7e34ce3dbc7ec7fb79e76ec5878e3c1da816f8f1
95940:S 03 May 2025 23:05:25.610 # DEBUG LOG: ========== I am replica 9 ==========
### Starting test Cluster is up in tests/unit/cluster/slave-selection.tcl
### Starting test The first master has actually 5 slaves in tests/unit/cluster/slave-selection.tcl
### Starting test Slaves of #0 are instance #3, #6, #9, #12 and #15 as expected in tests/unit/cluster/slave-selection.tcl
### Starting test Instance #3, #6, #9, #12 and #15 synced with the master in tests/unit/cluster/slave-selection.tcl
### Starting test New Master down consecutively in tests/unit/cluster/slave-selection.tcl
95940:S 03 May 2025 23:05:37.104 * NODE 90fe983302002b82cb23590ee6d0f8941a6f292c () possibly failing.
95940:S 03 May 2025 23:05:37.259 * Node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () reported node 90fe983302002b82cb23590ee6d0f8941a6f292c () as not reachable.
95940:S 03 May 2025 23:05:37.260 * FAIL message received from b0460c495605a702515eee2bbc385790a6fbec4f () about 90fe983302002b82cb23590ee6d0f8941a6f292c ()
95940:S 03 May 2025 23:05:37.261 # Cluster state changed: fail
95940:S 03 May 2025 23:05:37.261 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
95940:S 03 May 2025 23:05:37.305 * Start of election delayed for 3552 milliseconds (rank #3, primary rank #0, offset 14).
95940:S 03 May 2025 23:05:37.406 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:37.607 * Node b0460c495605a702515eee2bbc385790a6fbec4f () reported node 90fe983302002b82cb23590ee6d0f8941a6f292c () as not reachable.
95940:S 03 May 2025 23:05:38.009 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:38.299 * Configuration change detected. Reconfiguring myself as a replica of node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:05:38.301 * Connection with primary lost.
95940:M 03 May 2025 23:05:38.306 * Caching the disconnected primary state.
95940:S 03 May 2025 23:05:38.309 * Connecting to PRIMARY 127.0.0.1:27152
95940:S 03 May 2025 23:05:38.314 * PRIMARY <-> REPLICA sync started
95940:S 03 May 2025 23:05:38.317 * Node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () reported node 90fe983302002b82cb23590ee6d0f8941a6f292c () as not reachable.
95940:S 03 May 2025 23:05:38.324 * Cluster state changed: ok
95940:S 03 May 2025 23:05:38.370 * Non blocking connect for SYNC fired the event.
95940:S 03 May 2025 23:05:38.370 * Node b07f064c60c460967f75f99083ccb3194b9c7693 () is now a replica of node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:38.370 * Node 46671e5b4c343c9639452ff6da1989e7ac8280cd () is now a replica of node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:38.370 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now a replica of node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:38.372 * Primary replied to PING, replication can continue...
95940:S 03 May 2025 23:05:38.372 * Trying a partial resynchronization (request 7d58ddc376db3b26eb97d51b67f28be16101cc7d:15).
95940:S 03 May 2025 23:05:38.373 * Successful partial resynchronization with primary.
95940:S 03 May 2025 23:05:38.373 * Primary replication ID changed to 7fb7dac86b097a0c131ecb94802c8a538e89786c
95940:S 03 May 2025 23:05:38.373 * PRIMARY <-> REPLICA sync: Primary accepted a Partial Resynchronization.
95940:S 03 May 2025 23:05:41.643 * NODE 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () possibly failing.
95940:S 03 May 2025 23:05:41.677 * Node b0460c495605a702515eee2bbc385790a6fbec4f () reported node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () as not reachable.
95940:S 03 May 2025 23:05:41.696 * FAIL message received from 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () about 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca ()
95940:S 03 May 2025 23:05:41.697 # Cluster state changed: fail
95940:S 03 May 2025 23:05:41.697 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
95940:S 03 May 2025 23:05:41.745 * Start of election delayed for 2705 milliseconds (rank #2, primary rank #0, offset 14).
95940:S 03 May 2025 23:05:41.846 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:41.846 * Node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () reported node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () as not reachable.
95940:S 03 May 2025 23:05:42.046 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:42.498 * Configuration change detected. Reconfiguring myself as a replica of node 46671e5b4c343c9639452ff6da1989e7ac8280cd () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:05:42.501 * Connection with primary lost.
95940:M 03 May 2025 23:05:42.505 * Caching the disconnected primary state.
95940:S 03 May 2025 23:05:42.514 * Connecting to PRIMARY 127.0.0.1:27155
95940:S 03 May 2025 23:05:42.514 * PRIMARY <-> REPLICA sync started
95940:S 03 May 2025 23:05:42.514 * Cluster state changed: ok
95940:S 03 May 2025 23:05:42.569 * Non blocking connect for SYNC fired the event.
95940:S 03 May 2025 23:05:42.572 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now a replica of node 46671e5b4c343c9639452ff6da1989e7ac8280cd () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:42.576 * Node b07f064c60c460967f75f99083ccb3194b9c7693 () is now a replica of node 46671e5b4c343c9639452ff6da1989e7ac8280cd () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:42.578 * Primary replied to PING, replication can continue...
95940:S 03 May 2025 23:05:42.579 * Trying a partial resynchronization (request 7fb7dac86b097a0c131ecb94802c8a538e89786c:15).
95940:S 03 May 2025 23:05:42.585 * Successful partial resynchronization with primary.
95940:S 03 May 2025 23:05:42.585 * Primary replication ID changed to 0113593cb42adf4990c7f792c83eed6462d4fb09
95940:S 03 May 2025 23:05:42.585 * PRIMARY <-> REPLICA sync: Primary accepted a Partial Resynchronization.
95940:S 03 May 2025 23:05:42.676 * Node 46671e5b4c343c9639452ff6da1989e7ac8280cd () reported node 90fe983302002b82cb23590ee6d0f8941a6f292c () as not reachable.
95940:S 03 May 2025 23:05:45.903 * NODE 46671e5b4c343c9639452ff6da1989e7ac8280cd () possibly failing.
95940:S 03 May 2025 23:05:46.133 * Node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () reported node 46671e5b4c343c9639452ff6da1989e7ac8280cd () as not reachable.
95940:S 03 May 2025 23:05:46.133 * Node b0460c495605a702515eee2bbc385790a6fbec4f () reported node 46671e5b4c343c9639452ff6da1989e7ac8280cd () as not reachable.
95940:S 03 May 2025 23:05:46.133 * Marking node 46671e5b4c343c9639452ff6da1989e7ac8280cd () as failing (quorum reached).
95940:S 03 May 2025 23:05:46.133 # Cluster state changed: fail
95940:S 03 May 2025 23:05:46.133 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
95940:S 03 May 2025 23:05:46.205 * Start of election delayed for 1729 milliseconds (rank #1, primary rank #0, offset 14).
95940:S 03 May 2025 23:05:46.306 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:46.848 * Configuration change detected. Reconfiguring myself as a replica of node b07f064c60c460967f75f99083ccb3194b9c7693 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:05:46.851 * Connection with primary lost.
95940:M 03 May 2025 23:05:46.853 * Caching the disconnected primary state.
95940:S 03 May 2025 23:05:46.854 * Connecting to PRIMARY 127.0.0.1:27146
95940:S 03 May 2025 23:05:46.854 * PRIMARY <-> REPLICA sync started
95940:S 03 May 2025 23:05:46.854 * Cluster state changed: ok
95940:S 03 May 2025 23:05:46.895 * Non blocking connect for SYNC fired the event.
95940:S 03 May 2025 23:05:46.900 * Primary replied to PING, replication can continue...
95940:S 03 May 2025 23:05:46.901 * Trying a partial resynchronization (request 0113593cb42adf4990c7f792c83eed6462d4fb09:15).
95940:S 03 May 2025 23:05:46.904 * Successful partial resynchronization with primary.
95940:S 03 May 2025 23:05:46.904 * Primary replication ID changed to 6fb89925db61d14916250baed69f689f8d2c0368
95940:S 03 May 2025 23:05:46.904 * PRIMARY <-> REPLICA sync: Primary accepted a Partial Resynchronization.
95940:S 03 May 2025 23:05:46.907 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now a replica of node b07f064c60c460967f75f99083ccb3194b9c7693 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:05:46.910 * Node b07f064c60c460967f75f99083ccb3194b9c7693 () reported node 46671e5b4c343c9639452ff6da1989e7ac8280cd () as not reachable.
95940:S 03 May 2025 23:05:50.337 * NODE b07f064c60c460967f75f99083ccb3194b9c7693 () possibly failing.
95940:S 03 May 2025 23:05:50.368 * Node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () reported node b07f064c60c460967f75f99083ccb3194b9c7693 () as not reachable.
95940:S 03 May 2025 23:05:50.425 * FAIL message received from b0460c495605a702515eee2bbc385790a6fbec4f () about b07f064c60c460967f75f99083ccb3194b9c7693 ()
95940:S 03 May 2025 23:05:50.429 # Cluster state changed: fail
95940:S 03 May 2025 23:05:50.432 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
95940:S 03 May 2025 23:05:50.458 * Start of election delayed for 691 milliseconds (rank #0, primary rank #0, offset 14).
95940:S 03 May 2025 23:05:50.562 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:51.070 * Currently unable to failover: Waiting the delay before I can start a new failover.
95940:S 03 May 2025 23:05:51.172 * Starting a failover election for epoch 21, node config epoch is 20
95940:S 03 May 2025 23:05:51.229 * Currently unable to failover: Waiting for votes, but majority still not reached.
95940:S 03 May 2025 23:05:51.229 * Needed quorum: 2. Number of votes received so far: 1
95940:S 03 May 2025 23:05:51.231 * Failover election won: I'm the new primary.
95940:S 03 May 2025 23:05:51.231 * configEpoch set to 21 after successful failover
95940:S 03 May 2025 23:05:51.231 * Setting myself to primary in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8 after failover; my old primary is b07f064c60c460967f75f99083ccb3194b9c7693 ()
95940:M 03 May 2025 23:05:51.231 * Connection with primary lost.
95940:M 03 May 2025 23:05:51.231 * Caching the disconnected primary state.
95940:M 03 May 2025 23:05:51.231 * Discarding previously cached primary state.
95940:M 03 May 2025 23:05:51.231 * Setting secondary replication ID to 6fb89925db61d14916250baed69f689f8d2c0368, valid up to offset: 15. New replication ID is 2d3ac1003838d318ad1f80e73c98b3df8fea7912
95940:M 03 May 2025 23:05:51.231 * Cluster state changed: ok
95940:M 03 May 2025 23:05:51.318 - Accepted 127.0.0.1:60179
95940:M 03 May 2025 23:05:51.375 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now a replica of node b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:05:51.386 * Replica 127.0.0.1:27143 asks for synchronization
95940:M 03 May 2025 23:05:51.387 * Partial resynchronization request from 127.0.0.1:27143 accepted. Sending 0 bytes of backlog starting from offset 15.
95940:M 03 May 2025 23:05:51.400 * Node b0460c495605a702515eee2bbc385790a6fbec4f () reported node b07f064c60c460967f75f99083ccb3194b9c7693 () as not reachable.
### Starting test Check for memory leaks (pid 96528) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96479) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96430) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96377) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96326) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96171) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96119) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 96051) in tests/unit/cluster/slave-selection.tcl
### Starting test Check for memory leaks (pid 95992) in tests/unit/cluster/slave-selection.tcl
95940:signal-handler (1746281169) Received SIGTERM scheduling shutdown...
95940:M 03 May 2025 23:06:09.537 * User requested shutdown...
95940:M 03 May 2025 23:06:09.537 * Waiting for replicas before shutting down.
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61026
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61032
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61059
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61062
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61063
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61064
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61065
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61066
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61067
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61068
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61069
95940:M 03 May 2025 23:06:09.537 - Accepting cluster node connection from 127.0.0.1:61075
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:61076
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62051
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62053
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62079
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62083
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62084
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62093
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62094
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62095
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62096
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62097
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62098
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62099
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:62100
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63221
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63236
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63276
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63281
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63294
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63296
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63304
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63307
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63309
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63336
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:63341
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:64915
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:64963
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:64985
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:64990
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:64998
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:65001
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:65011
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:65016
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:65021
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:65025
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:65034
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50422
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50442
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50464
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50475
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50493
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50496
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50513
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50516
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:50534
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53286
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53327
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53385
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53392
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53401
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53437
95940:M 03 May 2025 23:06:09.538 - Accepting cluster node connection from 127.0.0.1:53477
95940:M 03 May 2025 23:06:09.538 - Node 90fe983302002b82cb23590ee6d0f8941a6f292c has old slots configuration, sending an UPDATE message about b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3
95940:M 03 May 2025 23:06:09.538 * A failover occurred in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8; node 90fe983302002b82cb23590ee6d0f8941a6f292c () failed over to node b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3 () with a config epoch of 21
95940:M 03 May 2025 23:06:09.538 * Node 90fe983302002b82cb23590ee6d0f8941a6f292c () is now a replica of node b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:06:09.538 - Accepted 127.0.0.1:62349
95940:M 03 May 2025 23:06:09.538 - Accepted 127.0.0.1:62351
95940:M 03 May 2025 23:06:09.538 - Node 46671e5b4c343c9639452ff6da1989e7ac8280cd has old slots configuration, sending an UPDATE message about b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3
95940:M 03 May 2025 23:06:09.538 * A failover occurred in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8; node 46671e5b4c343c9639452ff6da1989e7ac8280cd () failed over to node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () with a config epoch of 22
95940:M 03 May 2025 23:06:09.538 * Node 46671e5b4c343c9639452ff6da1989e7ac8280cd () is now a replica of node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:06:09.539 - Node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca has old slots configuration, sending an UPDATE message about b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3
95940:M 03 May 2025 23:06:09.539 * A failover occurred in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8; node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () failed over to node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () with a config epoch of 22
95940:M 03 May 2025 23:06:09.539 * Node 42c7847e7a18321cb1983bca0f6288e8e5f0a2ca () is now a replica of node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:06:09.539 - Client closed connection id=3 addr=127.0.0.1:49548 laddr=127.0.0.1:27149 fd=14 name= age=50 idle=18 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=8192 rbp=0 obl=0 oll=0 omem=0 tot-mem=25600 events=r cmd=info user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=4664 tot-net-out=1898182 tot-cmds=315
95940:M 03 May 2025 23:06:09.539 - Reading from client: Connection reset by peer
95940:M 03 May 2025 23:06:09.539 * Connection with replica client id #7 lost.
95940:M 03 May 2025 23:06:09.539 * Ignore stale message from fdccb5fdeea48c4647d42af59fa9a489b564cd74 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8; gossip config epoch: 21, current config epoch: 22
95940:M 03 May 2025 23:06:09.539 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () is now a replica of node b17ef56c324fd943a2e1e7ac4de1ae88e6e2d2a3 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:M 03 May 2025 23:06:09.539 # Failover auth denied to fdccb5fdeea48c4647d42af59fa9a489b564cd74 () for epoch 22: its primary is up
95940:M 03 May 2025 23:06:09.540 * Configuration change detected. Reconfiguring myself as a replica of node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:06:09.540 * Before turning into a replica, using my own primary parameters to synthesize a cached primary: I may be able to synchronize with the new primary with just a partial transfer.
95940:S 03 May 2025 23:06:09.540 * Connecting to PRIMARY 127.0.0.1:27143
95940:S 03 May 2025 23:06:09.540 * PRIMARY <-> REPLICA sync started
95940:S 03 May 2025 23:06:09.540 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () reported node b07f064c60c460967f75f99083ccb3194b9c7693 () as not reachable.
95940:S 03 May 2025 23:06:09.540 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () reported node 46671e5b4c343c9639452ff6da1989e7ac8280cd () as not reachable.
95940:S 03 May 2025 23:06:09.540 * Node 046e44a65cc052012ccd55765babd4aabb14ecc1 () is now a replica of node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () in shard ce68b6e8cae6a92e5c5c44660f20acbe299591a8
95940:S 03 May 2025 23:06:09.540 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () reported node b0460c495605a702515eee2bbc385790a6fbec4f () as not reachable.
95940:S 03 May 2025 23:06:09.540 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () reported node 13a4d5b2dd0fe1636912ced49ca4e51bd44c4fa2 () as not reachable.
95940:S 03 May 2025 23:06:09.540 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () reported node b9b46fb4ad2ec44f60cdb6c0ca604a7f1f256726 () as not reachable.
95940:S 03 May 2025 23:06:09.540 * Node fdccb5fdeea48c4647d42af59fa9a489b564cd74 () reported node 5efe9ebd92f2876d4c3e37c7dc016ea9d86346eb () as not reachable.
95940:S 03 May 2025 23:06:09.540 - Client closed connection id=8 addr=127.0.0.1:62349 laddr=127.0.0.1:27149 fd=15 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=NULL user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=0 tot-net-out=0 tot-cmds=0
95940:S 03 May 2025 23:06:09.548 - Client closed connection id=9 addr=127.0.0.1:62351 laddr=127.0.0.1:27149 fd=97 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=14 tot-net-out=7 tot-cmds=1
95940:S 03 May 2025 23:06:09.548 * Non blocking connect for SYNC fired the event.
95940:S 03 May 2025 23:06:09.549 * Primary replied to PING, replication can continue...
95940:S 03 May 2025 23:06:09.549 * Trying a partial resynchronization (request 2d3ac1003838d318ad1f80e73c98b3df8fea7912:52).
95940:S 03 May 2025 23:06:09.549 * Full resync from primary: d58996b7e7b5d8510f805202c959696ec73dee74:28
95940:S 03 May 2025 23:06:09.551 * PRIMARY <-> REPLICA sync: receiving streamed RDB from primary with EOF to disk
95940:S 03 May 2025 23:06:09.554 * Discarding previously cached primary state.
95940:S 03 May 2025 23:06:09.554 * PRIMARY <-> REPLICA sync: Flushing old data
95940:S 03 May 2025 23:06:09.554 * PRIMARY <-> REPLICA sync: Loading DB in memory
95940:S 03 May 2025 23:06:09.554 * Loading RDB produced by Valkey version 255.255.255
95940:S 03 May 2025 23:06:09.554 * RDB age 0 seconds
95940:S 03 May 2025 23:06:09.554 * RDB memory usage when created 3.05 Mb
95940:S 03 May 2025 23:06:09.554 * Done loading RDB, keys loaded: 0, keys expired: 0.
95940:S 03 May 2025 23:06:09.554 * PRIMARY <-> REPLICA sync: Finished with success
95940:S 03 May 2025 23:06:09.638 * Removing the pid file.
95940:S 03 May 2025 23:06:09.638 * Saving the cluster configuration file before exiting.
95940:S 03 May 2025 23:06:09.643 * Removing the unix socket file.
95940:S 03 May 2025 23:06:09.643 # Valkey is now ready to exit, bye bye...
