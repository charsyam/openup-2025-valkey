### Starting server for test 
93748:M 03 May 2025 23:04:55.959 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
93748:M 03 May 2025 23:04:55.959 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
93748:M 03 May 2025 23:04:55.959 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=93748, just started
93748:M 03 May 2025 23:04:55.959 * Configuration loaded
93748:M 03 May 2025 23:04:55.959 * Increased maximum number of open files to 10032 (it was originally set to 256).
93748:M 03 May 2025 23:04:55.959 * monotonic clock: POSIX clock_gettime
93748:M 03 May 2025 23:04:55.959 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 23639
 |###|   .+###############+.   |###|    PID: 93748                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

93748:M 03 May 2025 23:04:55.960 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
93748:M 03 May 2025 23:04:55.960 * No cluster configuration found, I'm cca93a04ed33746869c86c50fb722d75e6455a3c
93748:M 03 May 2025 23:04:55.964 * Server initialized
93748:M 03 May 2025 23:04:55.964 * Ready to accept connections tcp
93748:M 03 May 2025 23:04:55.964 * Ready to accept connections unix
93748:M 03 May 2025 23:04:56.102 - Accepted 127.0.0.1:52353
93748:M 03 May 2025 23:04:56.103 - Client closed connection id=2 addr=127.0.0.1:52353 laddr=127.0.0.1:23639 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
93748:M 03 May 2025 23:04:56.111 - Accepted 127.0.0.1:52359
93748:M 03 May 2025 23:04:56.271 # Missing implement of connection type tls
93748:M 03 May 2025 23:04:56.363 - Accepting cluster node connection from 127.0.0.1:52436
93748:M 03 May 2025 23:04:56.363 * IP address for this node updated to 127.0.0.1
93748:M 03 May 2025 23:04:56.469 * Successfully completed handshake with 0edc1598c6f19b54b438275a7178c8e2fb86866e ()
93748:M 03 May 2025 23:04:56.469 * Successfully completed handshake with 2a61950352228d652e0cd28d607003203413edc3 ()
93748:M 03 May 2025 23:04:56.477 * Successfully completed handshake with 9a21967532f5555297f32e59a6d14132c2074d4c ()
93748:M 03 May 2025 23:04:56.498 - Accepting cluster node connection from 127.0.0.1:52486
93748:M 03 May 2025 23:04:56.568 - Accepting cluster node connection from 127.0.0.1:52500
93748:M 03 May 2025 23:04:56.571 - Accepting cluster node connection from 127.0.0.1:52504
93748:M 03 May 2025 23:04:56.574 - Accepting cluster node connection from 127.0.0.1:52506
93748:M 03 May 2025 23:04:56.574 - Accepting cluster node connection from 127.0.0.1:52509
93748:M 03 May 2025 23:04:56.605 - Accepting cluster node connection from 127.0.0.1:52511
93748:M 03 May 2025 23:04:56.610 - Accepting cluster node connection from 127.0.0.1:52527
93748:M 03 May 2025 23:04:56.612 - Accepting cluster node connection from 127.0.0.1:52534
93748:M 03 May 2025 23:04:56.674 - Accepting cluster node connection from 127.0.0.1:52548
93748:M 03 May 2025 23:04:56.681 - Accepting cluster node connection from 127.0.0.1:52553
93748:M 03 May 2025 23:04:56.736 - Accepting cluster node connection from 127.0.0.1:52569
93748:M 03 May 2025 23:04:56.742 - Accepting cluster node connection from 127.0.0.1:52586
93748:M 03 May 2025 23:04:56.840 - Accepting cluster node connection from 127.0.0.1:52612
93748:M 03 May 2025 23:04:56.885 * configEpoch collision with node d74093fa5cbee191e5f4875844632e4b1178a823 (). configEpoch set to 7
93748:M 03 May 2025 23:04:56.957 * configEpoch collision with node d74093fa5cbee191e5f4875844632e4b1178a823 (). configEpoch set to 8
93748:M 03 May 2025 23:04:57.220 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is no longer primary of shard 1d78e2e0763c8560fbddbec241a38e7f3805fcca; removed all 0 slot(s) it used to own
93748:M 03 May 2025 23:04:57.227 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is now part of shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93748:M 03 May 2025 23:04:57.230 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is now a replica of node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93748:M 03 May 2025 23:04:57.245 - Accepted 127.0.0.1:52692
93748:M 03 May 2025 23:04:57.276 * Node 0edc1598c6f19b54b438275a7178c8e2fb86866e () is no longer primary of shard 4fc121ae00c8fcb15da30f003ea199f2f8cfe457; removed all 0 slot(s) it used to own
93748:M 03 May 2025 23:04:57.277 * Node 0edc1598c6f19b54b438275a7178c8e2fb86866e () is now part of shard 7ca01ed584c7f696f5412b60ba3b31d3fccb3643
93748:M 03 May 2025 23:04:57.278 * Node 0edc1598c6f19b54b438275a7178c8e2fb86866e () is now a replica of node cca93a04ed33746869c86c50fb722d75e6455a3c () in shard 7ca01ed584c7f696f5412b60ba3b31d3fccb3643
93748:M 03 May 2025 23:04:57.308 * Node d74093fa5cbee191e5f4875844632e4b1178a823 () is no longer primary of shard a2c0be6204a680811ccd3d5db82f6cdf886409a3; removed all 0 slot(s) it used to own
93748:M 03 May 2025 23:04:57.310 * Node d74093fa5cbee191e5f4875844632e4b1178a823 () is now part of shard 4aa4b960efe84440273421442c9b7de1b3bf0707
93748:M 03 May 2025 23:04:57.310 * Node d74093fa5cbee191e5f4875844632e4b1178a823 () is now a replica of node 9a21967532f5555297f32e59a6d14132c2074d4c () in shard 4aa4b960efe84440273421442c9b7de1b3bf0707
93748:M 03 May 2025 23:04:57.321 * Replica 127.0.0.1:23634 asks for synchronization
93748:M 03 May 2025 23:04:57.329 * Full resync requested by replica 127.0.0.1:23634
93748:M 03 May 2025 23:04:57.329 * Replication backlog created, my new replication IDs are '3ab5308db0eeaf3fa66d5f7a46082e980dee5200' and '0000000000000000000000000000000000000000'
93748:M 03 May 2025 23:04:57.329 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
93748:M 03 May 2025 23:04:57.330 * Background RDB transfer started by pid 93865 to pipe through parent process
93865:C 03 May 2025 23:04:57.331 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
93748:M 03 May 2025 23:04:57.336 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
93748:M 03 May 2025 23:04:57.346 * Node 5c328f9d2f54be47ee2e8999022778ff086b3624 () is no longer primary of shard 79a63df7d6cad403d83b5acc455de387c1311a74; removed all 0 slot(s) it used to own
93748:M 03 May 2025 23:04:57.347 * Node 5c328f9d2f54be47ee2e8999022778ff086b3624 () is now part of shard 06916f579a38286ea2028a011368696032f6c8ac
93748:M 03 May 2025 23:04:57.354 * Node 5c328f9d2f54be47ee2e8999022778ff086b3624 () is now a replica of node 2a61950352228d652e0cd28d607003203413edc3 () in shard 06916f579a38286ea2028a011368696032f6c8ac
93748:M 03 May 2025 23:04:57.375 * Node db91efb42d98480add759e79b4ee28dd04a3a2e0 () is no longer primary of shard 809fefde03e09cb34c1e2b85c6e639e885dc16db; removed all 0 slot(s) it used to own
93748:M 03 May 2025 23:04:57.380 * Node db91efb42d98480add759e79b4ee28dd04a3a2e0 () is now part of shard 54d1839130debb218475572bcaa4e761f3b9b2e4
93748:M 03 May 2025 23:04:57.382 * Node db91efb42d98480add759e79b4ee28dd04a3a2e0 () is now a replica of node d56e5afcc74e6566efeca03c7251faf1ab21a425 () in shard 54d1839130debb218475572bcaa4e761f3b9b2e4
93748:M 03 May 2025 23:04:57.394 * Background RDB transfer terminated with success
93748:M 03 May 2025 23:04:57.395 * Streamed RDB transfer with replica 127.0.0.1:23634 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
93748:M 03 May 2025 23:04:57.401 * Synchronization with replica 127.0.0.1:23634 succeeded
93748:M 03 May 2025 23:04:57.416 # DEBUG LOG: ========== I am primary 1 ==========
93748:M 03 May 2025 23:04:58.019 * Cluster state changed: ok
### Starting test Cluster is up in tests/unit/cluster/update-msg.tcl
### Starting test Cluster is writable in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:06.849 - Accepted 127.0.0.1:56062
93748:M 03 May 2025 23:05:06.895 - Client closed connection id=13 addr=127.0.0.1:56062 laddr=127.0.0.1:23639 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=1671 tot-net-out=732 tot-cmds=42
### Starting test Instance #5 is a slave in tests/unit/cluster/update-msg.tcl
### Starting test Instance #5 synced with the master in tests/unit/cluster/update-msg.tcl
### Starting test Killing one master node in tests/unit/cluster/update-msg.tcl
### Starting test Wait for failover in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:10.174 * Node 9a21967532f5555297f32e59a6d14132c2074d4c () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () as not reachable.
93748:M 03 May 2025 23:05:10.179 * Node 2a61950352228d652e0cd28d607003203413edc3 () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () as not reachable.
93748:M 03 May 2025 23:05:10.185 * Marking node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () as failing (quorum reached).
93748:M 03 May 2025 23:05:10.186 # Cluster state changed: fail
93748:M 03 May 2025 23:05:10.186 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
93748:M 03 May 2025 23:05:10.226 * Node d56e5afcc74e6566efeca03c7251faf1ab21a425 () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () as not reachable.
93748:M 03 May 2025 23:05:10.854 * Failover auth granted to 5c935c3f58573c3c0a91c32b85a9758afa06799e () for epoch 10
93748:M 03 May 2025 23:05:10.882 * Cluster state changed: ok
### Starting test Cluster should eventually be up again in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:10.923 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () as not reachable.
### Starting test Cluster is writable in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:11.107 - Accepted 127.0.0.1:59997
93748:M 03 May 2025 23:05:11.107 - Client closed connection id=14 addr=127.0.0.1:59997 laddr=127.0.0.1:23639 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=cluster|nodes user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=28 tot-net-out=1224 tot-cmds=1
93748:M 03 May 2025 23:05:11.109 - Accepted 127.0.0.1:60018
93748:M 03 May 2025 23:05:11.153 - Client closed connection id=15 addr=127.0.0.1:60018 laddr=127.0.0.1:23639 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=30 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=1671 tot-net-out=732 tot-cmds=42
### Starting test Instance #5 is now a master in tests/unit/cluster/update-msg.tcl
### Starting test Killing the new master #5 in tests/unit/cluster/update-msg.tcl
### Starting test Cluster should be down now in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:11.233 - DB 0: 21 keys (0 volatile) in 147 slots HT.
93748:M 03 May 2025 23:05:14.388 * Node 9a21967532f5555297f32e59a6d14132c2074d4c () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93748:M 03 May 2025 23:05:14.391 * Node d56e5afcc74e6566efeca03c7251faf1ab21a425 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93748:M 03 May 2025 23:05:14.404 * Marking node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as failing (quorum reached).
93748:M 03 May 2025 23:05:14.418 # Cluster state changed: fail
93748:M 03 May 2025 23:05:14.418 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
93748:M 03 May 2025 23:05:14.684 * Node 2a61950352228d652e0cd28d607003203413edc3 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
### Starting test Restarting the old master node in tests/unit/cluster/update-msg.tcl
### Starting test Instance #0 gets converted into a slave in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:14.698 - Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 has old slots configuration, sending an UPDATE message about 5c935c3f58573c3c0a91c32b85a9758afa06799e
93748:M 03 May 2025 23:05:14.706 * Clear FAIL state for node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 (): primary without slots is reachable again.
93748:M 03 May 2025 23:05:14.706 * A failover occurred in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8; node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () failed over to node 5c935c3f58573c3c0a91c32b85a9758afa06799e () with a config epoch of 10
93748:M 03 May 2025 23:05:14.706 * Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () is now a replica of node 5c935c3f58573c3c0a91c32b85a9758afa06799e () in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
### Starting test Restarting the new master node in tests/unit/cluster/update-msg.tcl
### Starting test Cluster is up again in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:14.769 * Node d56e5afcc74e6566efeca03c7251faf1ab21a425 () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () is back online.
93748:M 03 May 2025 23:05:14.835 * Node 9a21967532f5555297f32e59a6d14132c2074d4c () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () is back online.
93748:M 03 May 2025 23:05:14.926 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () is back online.
93748:M 03 May 2025 23:05:14.998 * Node 2a61950352228d652e0cd28d607003203413edc3 () reported node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () is back online.
93748:M 03 May 2025 23:05:15.745 * Failover auth granted to 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () for epoch 11
93748:M 03 May 2025 23:05:15.775 * Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () as not reachable.
93748:M 03 May 2025 23:05:15.775 * Cluster state changed: ok
93748:M 03 May 2025 23:05:15.823 * A failover occurred in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8; node 5c935c3f58573c3c0a91c32b85a9758afa06799e () failed over to node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () with a config epoch of 11
93748:M 03 May 2025 23:05:15.828 * Node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is now a replica of node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () in shard ebb0ba4bcc3d89a575a739ac458cba1b3bad2be8
93748:M 03 May 2025 23:05:15.907 * Clear FAIL state for node 5c935c3f58573c3c0a91c32b85a9758afa06799e (): replica is reachable again.
93748:M 03 May 2025 23:05:15.969 * Node 2a61950352228d652e0cd28d607003203413edc3 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93748:M 03 May 2025 23:05:16.031 * Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
### Starting test Check for memory leaks (pid 93781) in tests/unit/cluster/update-msg.tcl
93748:M 03 May 2025 23:05:16.147 * Node 9a21967532f5555297f32e59a6d14132c2074d4c () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93748:M 03 May 2025 23:05:16.311 - DB 0: 21 keys (0 volatile) in 147 slots HT.
93748:M 03 May 2025 23:05:17.134 * Node d56e5afcc74e6566efeca03c7251faf1ab21a425 () reported node 5c935c3f58573c3c0a91c32b85a9758afa06799e () is back online.
93748:M 03 May 2025 23:05:18.129 - Connection with Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 at 127.0.0.1:33640 failed: Connection refused
93748:M 03 May 2025 23:05:18.190 - Client closed connection id=3 addr=127.0.0.1:52359 laddr=127.0.0.1:23639 fd=14 name= age=22 idle=3 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=18432 events=r cmd=cluster|info user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=4306 tot-net-out=101272 tot-cmds=156
93748:M 03 May 2025 23:05:18.229 - Connection with Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 at 127.0.0.1:33640 failed: Connection refused
93748:M 03 May 2025 23:05:18.330 - Connection with Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 at 127.0.0.1:33640 failed: Connection refused
93748:M 03 May 2025 23:05:18.431 - Connection with Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 at 127.0.0.1:33640 failed: Connection refused
93748:M 03 May 2025 23:05:19.394 - Connection with Node 0355953e79c569a4e5e657cf95ccaac850f3a1f4 at 127.0.0.1:33640 failed: Connection refused
93748:signal-handler (1746281119) Received SIGTERM scheduling shutdown...
93748:M 03 May 2025 23:05:19.494 * User requested shutdown...
93748:M 03 May 2025 23:05:19.495 * 1 of 1 replicas are in sync when shutting down.
93748:M 03 May 2025 23:05:19.495 * Removing the pid file.
93748:M 03 May 2025 23:05:19.498 * Saving the cluster configuration file before exiting.
93748:M 03 May 2025 23:05:19.519 * Removing the unix socket file.
93748:M 03 May 2025 23:05:19.519 # Valkey is now ready to exit, bye bye...
