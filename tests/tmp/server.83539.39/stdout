### Starting server for test 
96630:M 03 May 2025 23:05:22.988 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
96630:M 03 May 2025 23:05:22.988 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
96630:M 03 May 2025 23:05:22.988 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=96630, just started
96630:M 03 May 2025 23:05:22.988 * Configuration loaded
96630:M 03 May 2025 23:05:22.989 * Increased maximum number of open files to 10032 (it was originally set to 256).
96630:M 03 May 2025 23:05:22.993 * monotonic clock: POSIX clock_gettime
96630:M 03 May 2025 23:05:22.994 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 28130
 |###|   .+###############+.   |###|    PID: 96630                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

96630:M 03 May 2025 23:05:22.996 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
96630:M 03 May 2025 23:05:22.996 * No cluster configuration found, I'm 2c8e9b9ace519470139431052e08a1eb6fdae92b
96630:M 03 May 2025 23:05:23.004 * Server initialized
96630:M 03 May 2025 23:05:23.004 * Ready to accept connections tcp
96630:M 03 May 2025 23:05:23.004 * Ready to accept connections unix
96630:M 03 May 2025 23:05:23.192 - Accepted 127.0.0.1:52574
96630:M 03 May 2025 23:05:23.202 - Client closed connection id=2 addr=127.0.0.1:52574 laddr=127.0.0.1:28130 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
96630:M 03 May 2025 23:05:23.250 - Accepted 127.0.0.1:52683
96630:M 03 May 2025 23:05:25.024 # Cluster is currently down: I am part of a minority partition.
96630:M 03 May 2025 23:05:25.808 - Accepting cluster node connection from 127.0.0.1:54864
96630:M 03 May 2025 23:05:25.811 * IP address for this node updated to 127.0.0.1
96630:M 03 May 2025 23:05:25.963 * configEpoch collision with node de13e24339d4ff7acc1955a3ee606a3d19d76a45 (). configEpoch set to 1
96630:M 03 May 2025 23:05:25.965 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
96630:M 03 May 2025 23:05:26.033 - Accepting cluster node connection from 127.0.0.1:54996
96630:M 03 May 2025 23:05:26.097 - Accepting cluster node connection from 127.0.0.1:55050
96630:M 03 May 2025 23:05:26.134 - Accepting cluster node connection from 127.0.0.1:55059
96630:M 03 May 2025 23:05:26.219 - Accepting cluster node connection from 127.0.0.1:55083
96630:M 03 May 2025 23:05:26.297 - Accepting cluster node connection from 127.0.0.1:55139
96630:M 03 May 2025 23:05:26.336 # Missing implement of connection type tls
96630:M 03 May 2025 23:05:26.338 * Node 3d3a9aaa923ca77543710ff8b4a617c35c610fe1 () is no longer primary of shard 7c00b3d168f90a4a199626d3f7a1772268c8c996; removed all 0 slot(s) it used to own
96630:M 03 May 2025 23:05:26.338 * Node 3d3a9aaa923ca77543710ff8b4a617c35c610fe1 () is now part of shard 84c62b06411891110d35dee733ca3e288cdbd695
96630:M 03 May 2025 23:05:26.338 * Node 3d3a9aaa923ca77543710ff8b4a617c35c610fe1 () is now a replica of node de13e24339d4ff7acc1955a3ee606a3d19d76a45 () in shard 84c62b06411891110d35dee733ca3e288cdbd695
96630:S 03 May 2025 23:05:26.338 * Connecting to PRIMARY 127.0.0.1:28136
96630:S 03 May 2025 23:05:26.338 * PRIMARY <-> REPLICA sync started
96630:S 03 May 2025 23:05:26.338 * Cluster state changed: ok
96630:S 03 May 2025 23:05:26.338 * Non blocking connect for SYNC fired the event.
96630:S 03 May 2025 23:05:26.342 * Primary replied to PING, replication can continue...
96630:S 03 May 2025 23:05:26.345 * Node a457adc1f12967e01636eebc7be42e1f1c6a5161 () is no longer primary of shard 8d2f10fdbeec748f5cd732cb467c198b35d51499; removed all 0 slot(s) it used to own
96630:S 03 May 2025 23:05:26.348 * Node a457adc1f12967e01636eebc7be42e1f1c6a5161 () is now part of shard 0f8c2ef8a8a4c6a45a4898c66dcc1edee626bdfd
96630:S 03 May 2025 23:05:26.362 * Node a457adc1f12967e01636eebc7be42e1f1c6a5161 () is now a replica of node 4c8f7204763471d1b43a2021a9e3b0e972c77b93 () in shard 0f8c2ef8a8a4c6a45a4898c66dcc1edee626bdfd
96630:S 03 May 2025 23:05:26.388 * Partial resynchronization not possible (no cached primary)
96630:S 03 May 2025 23:05:26.393 * Full resync from primary: 74318b6738e501583741e2a5bb69584c371173b2:0
96630:S 03 May 2025 23:05:26.393 * Node 82218a66383f364fb8b5ad2e45a1926d58d091e5 () is no longer primary of shard 4997817dc7ccddab078b67f876d51a145ed86931; removed all 0 slot(s) it used to own
96630:S 03 May 2025 23:05:26.393 * Node 82218a66383f364fb8b5ad2e45a1926d58d091e5 () is now part of shard 12ecf07cb98495b494daaec988f483de09ffbff0
96630:S 03 May 2025 23:05:26.393 * Node 82218a66383f364fb8b5ad2e45a1926d58d091e5 () is now a replica of node c2c6ca43fa4c24228277739956fea1b7c183d841 () in shard 12ecf07cb98495b494daaec988f483de09ffbff0
96630:S 03 May 2025 23:05:26.396 * PRIMARY <-> REPLICA sync: receiving streamed RDB from primary with EOF to disk
96630:S 03 May 2025 23:05:26.399 * PRIMARY <-> REPLICA sync: Flushing old data
96630:S 03 May 2025 23:05:26.400 * PRIMARY <-> REPLICA sync: Loading DB in memory
96630:S 03 May 2025 23:05:26.400 * Loading RDB produced by Valkey version 255.255.255
96630:S 03 May 2025 23:05:26.400 * RDB age 0 seconds
96630:S 03 May 2025 23:05:26.403 * RDB memory usage when created 2.97 Mb
96630:S 03 May 2025 23:05:26.403 * Done loading RDB, keys loaded: 0, keys expired: 0.
96630:S 03 May 2025 23:05:26.403 * PRIMARY <-> REPLICA sync: Finished with success
96630:S 03 May 2025 23:05:26.403 # DEBUG LOG: ========== I am replica 6 ==========
### Starting test auto-failover-on-shutdown hands over primaryship to a fully sync'd replica - shutdown - shutdown-timeout: 0 in tests/unit/cluster/auto-failover-on-shutdown.tcl
96630:S 03 May 2025 23:05:37.021 * Forced failover primary request accepted (primary request from 'id=5 addr=127.0.0.1:28136 laddr=127.0.0.1:55170 fd=27 name= user=(superuser) lib-name= lib-ver=').
96630:S 03 May 2025 23:05:37.021 * Start of election delayed for 0 milliseconds (rank #0, primary rank #0, offset 451).
96630:S 03 May 2025 23:05:37.021 * Starting a failover election for epoch 7, node config epoch is 0
96630:S 03 May 2025 23:05:37.033 - Client closed connection id=5 addr=127.0.0.1:28136 laddr=127.0.0.1:55170 fd=27 name= age=11 idle=0 flags=M capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=36 obl=0 oll=0 omem=0 tot-mem=18432 events=r cmd=cluster|failover user=(superuser) redir=-1 resp=2 lib-name= lib-ver= tot-net-in=451 tot-net-out=376 tot-cmds=13
96630:S 03 May 2025 23:05:37.033 * Connection with primary lost.
96630:S 03 May 2025 23:05:37.033 * Caching the disconnected primary state.
96630:S 03 May 2025 23:05:37.034 * Reconnecting to PRIMARY 127.0.0.1:28136
96630:S 03 May 2025 23:05:37.035 * PRIMARY <-> REPLICA sync started
96630:S 03 May 2025 23:05:37.035 * Currently unable to failover: Waiting for votes, but majority still not reached.
96630:S 03 May 2025 23:05:37.035 * Needed quorum: 2. Number of votes received so far: 1
96630:S 03 May 2025 23:05:37.035 # Error condition on socket for SYNC: Connection refused
96630:S 03 May 2025 23:05:37.035 * Failover election won: I'm the new primary.
96630:S 03 May 2025 23:05:37.035 * configEpoch set to 7 after successful failover
96630:S 03 May 2025 23:05:37.035 * Setting myself to primary in shard 84c62b06411891110d35dee733ca3e288cdbd695 after failover; my old primary is de13e24339d4ff7acc1955a3ee606a3d19d76a45 ()
96630:M 03 May 2025 23:05:37.035 * Discarding previously cached primary state.
96630:M 03 May 2025 23:05:37.035 * Setting secondary replication ID to 74318b6738e501583741e2a5bb69584c371173b2, valid up to offset: 452. New replication ID is 7f0c32dbd0479c5d29d1af8fd2bb7ad5040da450
### Starting test Unable to find a replica to perform an auto failover - shutdown in tests/unit/cluster/auto-failover-on-shutdown.tcl
96630:M 03 May 2025 23:05:37.107 - Accepted 127.0.0.1:56635
96630:M 03 May 2025 23:05:37.107 - Connection with Node de13e24339d4ff7acc1955a3ee606a3d19d76a45 at 127.0.0.1:38136 failed: Connection refused
96630:M 03 May 2025 23:05:37.114 * Node 3d3a9aaa923ca77543710ff8b4a617c35c610fe1 () is now a replica of node 2c8e9b9ace519470139431052e08a1eb6fdae92b () in shard 84c62b06411891110d35dee733ca3e288cdbd695
96630:M 03 May 2025 23:05:37.114 * Replica 127.0.0.1:28133 asks for synchronization
96630:M 03 May 2025 23:05:37.114 * Partial resynchronization request from 127.0.0.1:28133 accepted. Sending 0 bytes of backlog starting from offset 452.
96630:M 03 May 2025 23:05:37.128 * Connection with replica 127.0.0.1:28133 lost.
96630:M 03 May 2025 23:05:37.133 * User requested shutdown... (user request from 'id=3 addr=127.0.0.1:52683 laddr=127.0.0.1:28130 fd=14 name= user=default lib-name= lib-ver=')
96630:M 03 May 2025 23:05:37.135 * Removing the pid file.
96630:M 03 May 2025 23:05:37.139 * Unable to find a replica to perform the auto failover on shutdown.
96630:M 03 May 2025 23:05:37.148 * Saving the cluster configuration file before exiting.
96630:M 03 May 2025 23:05:37.166 * Removing the unix socket file.
96630:M 03 May 2025 23:05:37.167 # Valkey is now ready to exit, bye bye...
### Starting test Check for memory leaks (pid 96841) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 96810) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 96776) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 96722) in tests/unit/cluster/auto-failover-on-shutdown.tcl
### Starting test Check for memory leaks (pid 96676) in tests/unit/cluster/auto-failover-on-shutdown.tcl
