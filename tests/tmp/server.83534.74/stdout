### Starting server for test 
94002:M 03 May 2025 23:04:58.253 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
94002:M 03 May 2025 23:04:58.255 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
94002:M 03 May 2025 23:04:58.257 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=94002, just started
94002:M 03 May 2025 23:04:58.259 * Configuration loaded
94002:M 03 May 2025 23:04:58.265 * Increased maximum number of open files to 10032 (it was originally set to 256).
94002:M 03 May 2025 23:04:58.269 * monotonic clock: POSIX clock_gettime
94002:M 03 May 2025 23:04:58.270 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 25645
 |###|   .+###############+.   |###|    PID: 94002                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

94002:M 03 May 2025 23:04:58.284 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
94002:M 03 May 2025 23:04:58.288 * No cluster configuration found, I'm a0fd490cf9723cee3cccd519109d4722bf859964
94002:M 03 May 2025 23:04:58.297 * Server initialized
94002:M 03 May 2025 23:04:58.297 * Ready to accept connections tcp
94002:M 03 May 2025 23:04:58.297 * Ready to accept connections unix
94002:M 03 May 2025 23:04:58.392 - Accepted 127.0.0.1:52875
94002:M 03 May 2025 23:04:58.397 - Client closed connection id=2 addr=127.0.0.1:52875 laddr=127.0.0.1:25645 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
94002:M 03 May 2025 23:04:58.433 - Accepted 127.0.0.1:52876
94002:M 03 May 2025 23:04:58.686 # Missing implement of connection type tls
94002:M 03 May 2025 23:04:58.737 - Accepting cluster node connection from 127.0.0.1:52904
94002:M 03 May 2025 23:04:58.738 * IP address for this node updated to 127.0.0.1
94002:M 03 May 2025 23:04:58.879 - Accepting cluster node connection from 127.0.0.1:52936
94002:M 03 May 2025 23:04:58.886 - Accepting cluster node connection from 127.0.0.1:52938
94002:M 03 May 2025 23:04:58.941 - Accepting cluster node connection from 127.0.0.1:52946
94002:M 03 May 2025 23:04:58.943 - Accepting cluster node connection from 127.0.0.1:52952
94002:M 03 May 2025 23:04:58.949 * configEpoch collision with node ffeb2e425353840e689b63343f3a86dd9160a350 (). configEpoch set to 2
94002:M 03 May 2025 23:04:58.984 - Accepting cluster node connection from 127.0.0.1:52965
94002:M 03 May 2025 23:04:58.985 - Accepting cluster node connection from 127.0.0.1:52978
94002:M 03 May 2025 23:04:58.988 * Successfully completed handshake with 539e03f3a54b5b310857d0bc20d0efc58e44b516 ()
94002:M 03 May 2025 23:04:58.990 * configEpoch collision with node f7af5f6d568cbc58e57822057e3ffe67f09c514d (). configEpoch set to 3
94002:M 03 May 2025 23:04:59.035 * Successfully completed handshake with 69b67e3cd16549a48fb1805489fea1b89029b8b0 ()
94002:M 03 May 2025 23:04:59.040 - Handshake: we already know node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 (), updating the address if needed.
94002:M 03 May 2025 23:04:59.102 * configEpoch collision with node e5e04d1a171a8dea46269c46678b2b695adeef98 (). configEpoch set to 4
94002:M 03 May 2025 23:04:59.109 - Accepting cluster node connection from 127.0.0.1:53002
94002:M 03 May 2025 23:04:59.109 - Accepting cluster node connection from 127.0.0.1:53004
94002:M 03 May 2025 23:04:59.422 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is no longer primary of shard 24da2a8aea89efa9a2fdffc5931489628f33f81d; removed all 0 slot(s) it used to own
94002:M 03 May 2025 23:04:59.430 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is now part of shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94002:M 03 May 2025 23:04:59.434 * Node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () is now a replica of node 132625915118d530460e0bc9c2bf8c50e4efe6fe () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94002:M 03 May 2025 23:04:59.461 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is no longer primary of shard 81f13117c58aa63e271c14c8eaf541b802d5c634; removed all 0 slot(s) it used to own
94002:M 03 May 2025 23:04:59.464 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is now part of shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94002:M 03 May 2025 23:04:59.465 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is now a replica of node 132625915118d530460e0bc9c2bf8c50e4efe6fe () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94002:M 03 May 2025 23:04:59.470 - Accepted 127.0.0.1:53070
94002:M 03 May 2025 23:04:59.475 * Node 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 () is no longer primary of shard 8cd5286162bf8385161e7dc46c9b667742e2d497; removed all 0 slot(s) it used to own
94002:M 03 May 2025 23:04:59.476 * Node 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 () is now part of shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94002:M 03 May 2025 23:04:59.476 * Node 0b0a755830c9ce00a5c8d6af48c51335c90ddd26 () is now a replica of node a0fd490cf9723cee3cccd519109d4722bf859964 () in shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94002:M 03 May 2025 23:04:59.479 - Accepted 127.0.0.1:53071
94002:M 03 May 2025 23:04:59.479 * Node 539e03f3a54b5b310857d0bc20d0efc58e44b516 () is no longer primary of shard 61fc42c6623264f9314edde7e9e331590fe0176d; removed all 0 slot(s) it used to own
94002:M 03 May 2025 23:04:59.479 * Node 539e03f3a54b5b310857d0bc20d0efc58e44b516 () is now part of shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94002:M 03 May 2025 23:04:59.479 * Node 539e03f3a54b5b310857d0bc20d0efc58e44b516 () is now a replica of node a0fd490cf9723cee3cccd519109d4722bf859964 () in shard ee9718f2aae1b0bc76a424f9d129bc94cf4b9ff7
94002:M 03 May 2025 23:04:59.481 * Replica 127.0.0.1:25642 asks for synchronization
94002:M 03 May 2025 23:04:59.481 * Full resync requested by replica 127.0.0.1:25642
94002:M 03 May 2025 23:04:59.481 * Replication backlog created, my new replication IDs are '111671c56b43772e33790b9976defa6fca674672' and '0000000000000000000000000000000000000000'
94002:M 03 May 2025 23:04:59.481 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
94002:M 03 May 2025 23:04:59.485 * Background RDB transfer started by pid 94106 to pipe through parent process
94002:M 03 May 2025 23:04:59.485 * Node ffeb2e425353840e689b63343f3a86dd9160a350 () is no longer primary of shard e29b73940a49a08e3a8edc44d8b3e1b0da6bdeb7; removed all 0 slot(s) it used to own
94106:C 03 May 2025 23:04:59.486 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
94002:M 03 May 2025 23:04:59.487 * Node ffeb2e425353840e689b63343f3a86dd9160a350 () is now part of shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94002:M 03 May 2025 23:04:59.487 * Node ffeb2e425353840e689b63343f3a86dd9160a350 () is now a replica of node 69b67e3cd16549a48fb1805489fea1b89029b8b0 () in shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94002:M 03 May 2025 23:04:59.487 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
94002:M 03 May 2025 23:04:59.487 * Replica 127.0.0.1:25639 asks for synchronization
94002:M 03 May 2025 23:04:59.487 * Full resync requested by replica 127.0.0.1:25639
94002:M 03 May 2025 23:04:59.487 * Current BGSAVE has socket target. Waiting for next BGSAVE for SYNC
94002:M 03 May 2025 23:04:59.490 * Node f7af5f6d568cbc58e57822057e3ffe67f09c514d () is no longer primary of shard 89a8054a434e0f9cdc29c9f18862d2a67d9c68d9; removed all 0 slot(s) it used to own
94002:M 03 May 2025 23:04:59.491 * Node f7af5f6d568cbc58e57822057e3ffe67f09c514d () is now part of shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94002:M 03 May 2025 23:04:59.491 * Node f7af5f6d568cbc58e57822057e3ffe67f09c514d () is now a replica of node 69b67e3cd16549a48fb1805489fea1b89029b8b0 () in shard 1ec7bff35b80bcddac05a9ee4b22be6ba5583fbb
94002:M 03 May 2025 23:04:59.493 * Background RDB transfer terminated with success
94002:M 03 May 2025 23:04:59.493 * Streamed RDB transfer with replica 127.0.0.1:25642 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
94002:M 03 May 2025 23:04:59.493 * Synchronization with replica 127.0.0.1:25642 succeeded
94002:M 03 May 2025 23:04:59.493 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
94002:M 03 May 2025 23:04:59.494 * Background RDB transfer started by pid 94108 to pipe through parent process
94002:M 03 May 2025 23:04:59.494 # DEBUG LOG: ========== I am primary 1 ==========
94108:C 03 May 2025 23:04:59.494 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
94002:M 03 May 2025 23:04:59.495 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
94002:M 03 May 2025 23:04:59.498 * Background RDB transfer terminated with success
94002:M 03 May 2025 23:04:59.498 * Streamed RDB transfer with replica 127.0.0.1:25639 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
94002:M 03 May 2025 23:04:59.498 * Synchronization with replica 127.0.0.1:25639 succeeded
94002:M 03 May 2025 23:05:00.369 * Cluster state changed: ok
### Starting test Cluster is up in tests/unit/cluster/failover.tcl
### Starting test Cluster is writable in tests/unit/cluster/failover.tcl
94002:M 03 May 2025 23:05:09.190 - Accepted 127.0.0.1:58095
94002:M 03 May 2025 23:05:09.220 - Client closed connection id=11 addr=127.0.0.1:58095 laddr=127.0.0.1:25645 fd=33 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=16384 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=2471 tot-net-out=1082 tot-cmds=62
### Starting test Killing the first primary node in tests/unit/cluster/failover.tcl
### Starting test Wait for failover in tests/unit/cluster/failover.tcl
94002:M 03 May 2025 23:05:12.559 * FAIL message received from ffeb2e425353840e689b63343f3a86dd9160a350 () about 132625915118d530460e0bc9c2bf8c50e4efe6fe ()
94002:M 03 May 2025 23:05:12.562 # Cluster state changed: fail
94002:M 03 May 2025 23:05:12.562 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
94002:M 03 May 2025 23:05:12.611 * Node 69b67e3cd16549a48fb1805489fea1b89029b8b0 () reported node 132625915118d530460e0bc9c2bf8c50e4efe6fe () as not reachable.
94002:M 03 May 2025 23:05:13.273 * Failover auth granted to 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () for epoch 9
### Starting test Cluster should eventually be up again in tests/unit/cluster/failover.tcl
94002:M 03 May 2025 23:05:13.388 * Cluster state changed: ok
94002:M 03 May 2025 23:05:13.441 * Node e5e04d1a171a8dea46269c46678b2b695adeef98 () is now a replica of node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94002:M 03 May 2025 23:05:13.560 - DB 0: 31 keys (0 volatile) in 217 slots HT.
### Starting test Restarting the previously killed primary node in tests/unit/cluster/failover.tcl
### Starting test Instance #0 gets converted into a replica in tests/unit/cluster/failover.tcl
94002:M 03 May 2025 23:05:13.623 - Node 132625915118d530460e0bc9c2bf8c50e4efe6fe has old slots configuration, sending an UPDATE message about 4ff930e01affe2106176b50d1af5cd6bbf7a6462
94002:M 03 May 2025 23:05:13.649 * A failover occurred in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6; node 132625915118d530460e0bc9c2bf8c50e4efe6fe () failed over to node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () with a config epoch of 9
94002:M 03 May 2025 23:05:13.649 * Node 132625915118d530460e0bc9c2bf8c50e4efe6fe () is now a replica of node 4ff930e01affe2106176b50d1af5cd6bbf7a6462 () in shard 5fcc59e609fe3cf02a3a77a53c5b6e27076979d6
94002:M 03 May 2025 23:05:13.649 * Clear FAIL state for node 132625915118d530460e0bc9c2bf8c50e4efe6fe (): replica is reachable again.
### Starting test Make sure the replicas always get the different ranks in tests/unit/cluster/failover.tcl
### Starting test Check for memory leaks (pid 94021) in tests/unit/cluster/failover.tcl
94002:M 03 May 2025 23:05:13.974 * Node 69b67e3cd16549a48fb1805489fea1b89029b8b0 () reported node 132625915118d530460e0bc9c2bf8c50e4efe6fe () is back online.
94002:M 03 May 2025 23:05:16.198 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:16.215 - Client closed connection id=3 addr=127.0.0.1:52876 laddr=127.0.0.1:25645 fd=14 name= age=18 idle=3 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=760 obl=0 oll=0 omem=0 tot-mem=18432 events=r cmd=cluster|slots user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=2741 tot-net-out=60191 tot-cmds=98
94002:M 03 May 2025 23:05:16.299 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:16.400 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:16.502 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:16.604 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:16.705 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:16.806 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:M 03 May 2025 23:05:18.011 - Connection with Node 132625915118d530460e0bc9c2bf8c50e4efe6fe at 127.0.0.1:35646 failed: Connection refused
94002:signal-handler (1746281118) Received SIGTERM scheduling shutdown...
94002:M 03 May 2025 23:05:18.112 * User requested shutdown...
94002:M 03 May 2025 23:05:18.113 * 2 of 2 replicas are in sync when shutting down.
94002:M 03 May 2025 23:05:18.113 * Removing the pid file.
94002:M 03 May 2025 23:05:18.113 * Saving the cluster configuration file before exiting.
94002:M 03 May 2025 23:05:18.122 * Removing the unix socket file.
94002:M 03 May 2025 23:05:18.122 # Valkey is now ready to exit, bye bye...
