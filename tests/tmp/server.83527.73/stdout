### Starting server for test 
96264:M 03 May 2025 23:05:20.649 # WARNING: Changing databases number from 16 to 1 since we are in cluster mode
96264:M 03 May 2025 23:05:20.654 * oO0OoO0OoO0Oo Valkey is starting oO0OoO0OoO0Oo
96264:M 03 May 2025 23:05:20.665 * Valkey version=255.255.255, bits=64, commit=06ac4d81, modified=0, pid=96264, just started
96264:M 03 May 2025 23:05:20.677 * Configuration loaded
96264:M 03 May 2025 23:05:20.689 * Increased maximum number of open files to 10032 (it was originally set to 256).
96264:M 03 May 2025 23:05:20.697 * monotonic clock: POSIX clock_gettime
96264:M 03 May 2025 23:05:20.720 # Failed to write PID file: Permission denied
                .+^+.                                                
            .+#########+.                                            
        .+########+########+.           Valkey 255.255.255 (06ac4d81/0) 64 bit
    .+########+'     '+########+.                                    
 .########+'     .+.     '+########.    Running in cluster mode
 |####+'     .+#######+.     '+####|    Port: 22147
 |###|   .+###############+.   |###|    PID: 96264                     
 |###|   |#####*'' ''*#####|   |###|                                 
 |###|   |####'  .-.  '####|   |###|                                 
 |###|   |###(  (@@@)  )###|   |###|          https://valkey.io      
 |###|   |####.  '-'  .####|   |###|                                 
 |###|   |#####*.   .*#####|   |###|                                 
 |###|   '+#####|   |#####+'   |###|                                 
 |####+.     +##|   |#+'     .+####|                                 
 '#######+   |##|        .+########'                                 
    '+###|   |##|    .+########+'                                    
        '|   |####+########+'                                        
             +#########+'                                            
                '+v+'                                                

96264:M 03 May 2025 23:05:20.730 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
96264:M 03 May 2025 23:05:20.755 * No cluster configuration found, I'm fe8678675cc9fa5a6fedbade0150f28aff7c28b6
96264:M 03 May 2025 23:05:20.842 * Server initialized
96264:M 03 May 2025 23:05:20.858 * Ready to accept connections tcp
96264:M 03 May 2025 23:05:20.868 * Ready to accept connections unix
96264:M 03 May 2025 23:05:20.917 - Accepted 127.0.0.1:50638
96264:M 03 May 2025 23:05:20.928 - Client closed connection id=2 addr=127.0.0.1:50638 laddr=127.0.0.1:22147 fd=14 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=7 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=ping user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=7 tot-net-out=7 tot-cmds=1
96264:M 03 May 2025 23:05:20.973 - Accepted 127.0.0.1:50680
96264:M 03 May 2025 23:05:21.826 - Accepting cluster node connection from 127.0.0.1:51312
96264:M 03 May 2025 23:05:21.830 * IP address for this node updated to 127.0.0.1
96264:M 03 May 2025 23:05:21.965 * Successfully completed handshake with 588865ad810a9a2a0c032336635c16fcfda1ae87 ()
96264:M 03 May 2025 23:05:21.988 - Accepting cluster node connection from 127.0.0.1:51423
96264:M 03 May 2025 23:05:22.074 - Accepting cluster node connection from 127.0.0.1:51517
96264:M 03 May 2025 23:05:22.077 - Accepting cluster node connection from 127.0.0.1:51520
96264:M 03 May 2025 23:05:22.168 - Accepting cluster node connection from 127.0.0.1:51587
96264:M 03 May 2025 23:05:22.352 - Accepting cluster node connection from 127.0.0.1:51646
96264:M 03 May 2025 23:05:22.352 - Accepting cluster node connection from 127.0.0.1:51659
96264:M 03 May 2025 23:05:22.352 - Accepting cluster node connection from 127.0.0.1:51730
96264:M 03 May 2025 23:05:22.429 - Accepting cluster node connection from 127.0.0.1:51773
96264:M 03 May 2025 23:05:22.433 # Missing implement of connection type tls
96264:M 03 May 2025 23:05:22.786 * Node e2d292eade1e654dfcbaf0199c4085fd7be41119 () is no longer primary of shard 256f6dcdec4fe78f1a1e4e1da9762f32a8d53933; removed all 0 slot(s) it used to own
96264:M 03 May 2025 23:05:22.786 * Node e2d292eade1e654dfcbaf0199c4085fd7be41119 () is now part of shard 346ab41bb3203a84bc575e43be5ac11721355613
96264:M 03 May 2025 23:05:22.786 * Node e2d292eade1e654dfcbaf0199c4085fd7be41119 () is now a replica of node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () in shard 346ab41bb3203a84bc575e43be5ac11721355613
96264:M 03 May 2025 23:05:22.787 * Node cec11f5fa752abac24462ce05cbfbbcbf0f028f5 () is no longer primary of shard 938293520c68b674cb49633335610e25dd52713f; removed all 0 slot(s) it used to own
96264:M 03 May 2025 23:05:22.788 * Node cec11f5fa752abac24462ce05cbfbbcbf0f028f5 () is now part of shard c00ccb0e3de2bd2ae4befbae1ce2aca411aa8671
96264:M 03 May 2025 23:05:22.792 * Node cec11f5fa752abac24462ce05cbfbbcbf0f028f5 () is now a replica of node 588865ad810a9a2a0c032336635c16fcfda1ae87 () in shard c00ccb0e3de2bd2ae4befbae1ce2aca411aa8671
96264:M 03 May 2025 23:05:22.792 - Accepted 127.0.0.1:52034
96264:M 03 May 2025 23:05:22.792 * Node 48ce29c51389b534d0e756dbed048497c93bcc04 () is no longer primary of shard ce52f3ca12bf12223b54157bb8cdc9b3aab689a5; removed all 0 slot(s) it used to own
96264:M 03 May 2025 23:05:22.792 * Node 48ce29c51389b534d0e756dbed048497c93bcc04 () is now part of shard 7a9129f307605f9fc2e9e070fc3b91cb77372395
96264:M 03 May 2025 23:05:22.792 * Node 48ce29c51389b534d0e756dbed048497c93bcc04 () is now a replica of node fe8678675cc9fa5a6fedbade0150f28aff7c28b6 () in shard 7a9129f307605f9fc2e9e070fc3b91cb77372395
96264:M 03 May 2025 23:05:22.798 * Replica 127.0.0.1:22142 asks for synchronization
96264:M 03 May 2025 23:05:22.798 * Full resync requested by replica 127.0.0.1:22142
96264:M 03 May 2025 23:05:22.798 * Replication backlog created, my new replication IDs are '06f1681429d5d852058adc22c33bdbeb745ba86d' and '0000000000000000000000000000000000000000'
96264:M 03 May 2025 23:05:22.798 * Starting BGSAVE for SYNC with target: replicas sockets using: normal sync
96264:M 03 May 2025 23:05:22.799 * Background RDB transfer started by pid 96589 to pipe through parent process
96589:C 03 May 2025 23:05:22.801 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
96264:M 03 May 2025 23:05:22.805 * Diskless rdb transfer, done reading from pipe, 1 replicas still up.
96264:M 03 May 2025 23:05:22.848 * Background RDB transfer terminated with success
96264:M 03 May 2025 23:05:22.848 * Streamed RDB transfer with replica 127.0.0.1:22142 succeeded (socket). Waiting for REPLCONF ACK from replica to enable streaming
96264:M 03 May 2025 23:05:22.848 * Synchronization with replica 127.0.0.1:22142 succeeded
96264:M 03 May 2025 23:05:22.850 * Node a21e86dd7d6844054a7c3186a4d33006a7ef97bd () is no longer primary of shard 74397b408645fd96aa93c7a5707da778711f43d3; removed all 0 slot(s) it used to own
96264:M 03 May 2025 23:05:22.852 * Node a21e86dd7d6844054a7c3186a4d33006a7ef97bd () is now part of shard 1aacca3a9c7a750d76f6c800a57f4c1bd132846a
96264:M 03 May 2025 23:05:22.852 * Node a21e86dd7d6844054a7c3186a4d33006a7ef97bd () is now a replica of node 571ea23bf5cdffbe95dc1727682f4f31e6399d29 () in shard 1aacca3a9c7a750d76f6c800a57f4c1bd132846a
96264:M 03 May 2025 23:05:22.871 * Node a83b12f99ba80124eba788598806542c5da37d9e () is no longer primary of shard cf87ecc3efdcdfd3cac14e27b809f09893af48d2; removed all 0 slot(s) it used to own
96264:M 03 May 2025 23:05:22.871 * Node a83b12f99ba80124eba788598806542c5da37d9e () is now part of shard 3d0e9edcdf7533db82632130cb5a60794f38dd25
96264:M 03 May 2025 23:05:22.871 * Node a83b12f99ba80124eba788598806542c5da37d9e () is now a replica of node a59eb002dc5e0d938f7c474975db2c8b6a5620b0 () in shard 3d0e9edcdf7533db82632130cb5a60794f38dd25
96264:M 03 May 2025 23:05:22.872 # DEBUG LOG: ========== I am primary 2 ==========
96264:M 03 May 2025 23:05:22.949 * Cluster state changed: ok
### Starting test Cluster is up in tests/unit/cluster/manual-failover.tcl
### Starting test Cluster is writable in tests/unit/cluster/manual-failover.tcl
96264:M 03 May 2025 23:05:32.438 - Accepted 127.0.0.1:56464
96264:M 03 May 2025 23:05:32.501 - Client closed connection id=9 addr=127.0.0.1:56464 laddr=127.0.0.1:22147 fd=34 name= age=0 idle=0 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=16384 rbp=30 obl=0 oll=0 omem=0 tot-mem=33792 events=r cmd=get user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=1914 tot-net-out=838 tot-cmds=48
### Starting test Instance #5 is a slave in tests/unit/cluster/manual-failover.tcl
### Starting test Instance #5 synced with the master in tests/unit/cluster/manual-failover.tcl
### Starting test Make instance #0 unreachable without killing it in tests/unit/cluster/manual-failover.tcl
### Starting test Send CLUSTER FAILOVER to instance #5 in tests/unit/cluster/manual-failover.tcl
### Starting test Instance #5 is still a slave after some time (no failover) in tests/unit/cluster/manual-failover.tcl
96264:M 03 May 2025 23:05:35.719 * Node 588865ad810a9a2a0c032336635c16fcfda1ae87 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () as not reachable.
96264:M 03 May 2025 23:05:35.731 * Node a59eb002dc5e0d938f7c474975db2c8b6a5620b0 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () as not reachable.
96264:M 03 May 2025 23:05:35.732 * Marking node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () as failing (quorum reached).
96264:M 03 May 2025 23:05:35.736 # Cluster state changed: fail
96264:M 03 May 2025 23:05:35.741 # Cluster is currently down: At least one hash slot is not served by any available node. Please check the 'cluster-require-full-coverage' configuration.
96264:M 03 May 2025 23:05:35.792 * Node 571ea23bf5cdffbe95dc1727682f4f31e6399d29 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () as not reachable.
96264:M 03 May 2025 23:05:35.809 * Failover auth granted to e2d292eade1e654dfcbaf0199c4085fd7be41119 () for epoch 10
96264:M 03 May 2025 23:05:35.841 * Node e2d292eade1e654dfcbaf0199c4085fd7be41119 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () as not reachable.
96264:M 03 May 2025 23:05:35.844 * Cluster state changed: ok
96264:M 03 May 2025 23:05:36.317 - DB 0: 24 keys (0 volatile) in 168 slots HT.
### Starting test Wait for instance #0 to return back alive in tests/unit/cluster/manual-failover.tcl
96264:M 03 May 2025 23:05:41.361 - DB 0: 24 keys (0 volatile) in 168 slots HT.
96264:M 03 May 2025 23:05:42.579 - Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 has old slots configuration, sending an UPDATE message about e2d292eade1e654dfcbaf0199c4085fd7be41119
### Starting test Check for memory leaks (pid 96406) in tests/unit/cluster/manual-failover.tcl
96264:M 03 May 2025 23:05:42.586 * Clear FAIL state for node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 (): primary without slots is reachable again.
96264:M 03 May 2025 23:05:42.586 * A failover occurred in shard 346ab41bb3203a84bc575e43be5ac11721355613; node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () failed over to node e2d292eade1e654dfcbaf0199c4085fd7be41119 () with a config epoch of 10
96264:M 03 May 2025 23:05:42.586 * Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () is now a replica of node e2d292eade1e654dfcbaf0199c4085fd7be41119 () in shard 346ab41bb3203a84bc575e43be5ac11721355613
96264:M 03 May 2025 23:05:42.682 * Node a59eb002dc5e0d938f7c474975db2c8b6a5620b0 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () is back online.
96264:M 03 May 2025 23:05:42.773 * Node 571ea23bf5cdffbe95dc1727682f4f31e6399d29 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () is back online.
96264:M 03 May 2025 23:05:42.778 * Node 588865ad810a9a2a0c032336635c16fcfda1ae87 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () is back online.
96264:M 03 May 2025 23:05:42.852 * Node e2d292eade1e654dfcbaf0199c4085fd7be41119 () reported node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 () is back online.
96264:M 03 May 2025 23:05:43.581 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
### Starting test Check for memory leaks (pid 96354) in tests/unit/cluster/manual-failover.tcl
96264:M 03 May 2025 23:05:43.681 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:43.782 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:43.883 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:43.985 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.086 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.187 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.287 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.390 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.491 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.593 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.694 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.795 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.897 - Connection with Node 588865ad810a9a2a0c032336635c16fcfda1ae87 at 127.0.0.1:32148 failed: Connection refused
96264:M 03 May 2025 23:05:44.900 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:44.927 - Client closed connection id=3 addr=127.0.0.1:50680 laddr=127.0.0.1:22147 fd=14 name= age=24 idle=12 flags=N capa= db=0 sub=0 psub=0 ssub=0 multi=-1 watch=0 qbuf=0 qbuf-free=16890 argv-mem=0 multi-mem=0 rbs=1024 rbp=0 obl=0 oll=0 omem=0 tot-mem=18432 events=r cmd=cluster|info user=default redir=-1 resp=2 lib-name= lib-ver= tot-net-in=433 tot-net-out=13681 tot-cmds=13
96264:M 03 May 2025 23:05:44.997 - Connection with Node 588865ad810a9a2a0c032336635c16fcfda1ae87 at 127.0.0.1:32148 failed: Connection refused
96264:M 03 May 2025 23:05:44.999 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:45.099 - Connection with Node 588865ad810a9a2a0c032336635c16fcfda1ae87 at 127.0.0.1:32148 failed: Connection refused
96264:M 03 May 2025 23:05:45.099 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:45.200 - Connection with Node 588865ad810a9a2a0c032336635c16fcfda1ae87 at 127.0.0.1:32148 failed: Connection refused
96264:M 03 May 2025 23:05:45.202 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:M 03 May 2025 23:05:46.060 - Connection with Node 588865ad810a9a2a0c032336635c16fcfda1ae87 at 127.0.0.1:32148 failed: Connection refused
96264:M 03 May 2025 23:05:46.062 - Connection with Node eb36c71c6d2cd16ab0ad1c4e68c62360244596e2 at 127.0.0.1:32149 failed: Connection refused
96264:signal-handler (1746281146) Received SIGTERM scheduling shutdown...
96264:M 03 May 2025 23:05:46.161 * User requested shutdown...
96264:M 03 May 2025 23:05:46.165 * 1 of 1 replicas are in sync when shutting down.
96264:M 03 May 2025 23:05:46.167 * Removing the pid file.
96264:M 03 May 2025 23:05:46.169 * Saving the cluster configuration file before exiting.
96264:M 03 May 2025 23:05:46.190 * Removing the unix socket file.
96264:M 03 May 2025 23:05:46.190 # Valkey is now ready to exit, bye bye...
